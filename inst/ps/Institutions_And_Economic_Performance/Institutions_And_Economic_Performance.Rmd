
```{r 'check_ps', include=FALSE}

user.name = 'ENTER A USER NAME HERE'
```


## Exercise 1: Introduction

### Welcome!

I am happy to welcome you in this problem set elaborating on how the quality of institutions defined the current economic performance of many countries. This problem set is a part of my master thesis at the Ulm University and I tried to make it interesting, useful for learning econometric concepts and R at the same time. I hope you will enjoy the time spent going through the exercises and have fun!

### Content 

I bet that almost every person has asked herself a question that can be generally formulated as “Why some countries are rich whereas others are poor?”. Surely there were some debates with friends, family or even yourself, and surely there was hardly any certain answer that could convince everybody. This is quite normal as there is still little consensus on the answer to this question even among scientists and researchers, although there are some hypotheses, one of which we are going to touch in this problem set.

The paper *The Colonial Origins of Comparative Development: An Empirical Investigation* by Daron Acemoglu, Simon Johnson, and Jamea A. Robinson (2001) that can be downloaded <a href="https://www.aeaweb.org/articles?id=10.1257/aer.91.5.1369" target="_blank">here</a> delves deeper into the theory that institutions and property rights can be the fundamental cause of the large differences in income per capita across countries. The logic behind this hypothesis is that countries with better institutions, more secure property rights, and less distortionary policies will invest more in physical and human capital, and will use these factors more efficiently to achieve a greater level of income. Authors are analyzing variation in institutions in former European colonies and are accounting for different colonization strategies, the feasibility of settlements, and the persistence of established institutions. Of course, not everything is straightforward here, and there is enough space for two-way causation, endogeneity, and interpretation issues, and we will certainly touch on all these matters later. 

### Problem set 

The problem set is designed to lead you through the above-mentioned paper replicating the authors’ results in R. We will use some statistical and econometric methods and explore deeper the concept of instrumental variables estimation. You do not need to solve the exercises in the given order but it is recommended to do so as it makes more sense to start with descriptive statistics and go on with inference, causality, robustness checks, and to end with the conclusion. Moreover, later exercises expect earlier received knowledge from you. Within one tab you need to solve the tasks in the given order apart from the ones that are excluded explicitly with a note.

At every code chunk you will find some buttons:
- *edit*: click here to start entering your code. Once you have started to enter your code, you have new buttons to take use of. 
- *check*: press it to check your solution and run the code
- *run chunk*: press it to run the chunk and see the result directly in the chunk
- *data*: press it to show the data that is currently available for analysis and manipulations. It will open in a separate tab
- *hint*: press to to get a hint how to solve the task. This will help you in case something is unclear
- *solution*: press it to show the code you should receive as a result. I do not recommend using this option too often, as our goal is to get through all the tasks by solving them. However, you can always take advantage of it if you are stuck.

The problem set has the following structure:

**Exercise 1: Introduction**

**Exercise 2.1: Data overview**

**Exercise 2.2: Plotting the data**

**Exercise 2.3: Descriptive analysis & bivariate relations**

**Exercise 3.1: OLS regression**

**Exercise 3.2: Bias, endogeneity & control variables**

**Exercise 4.1: Instruments against endogeneity**

**Exercise 4.2: Choice of the plausible instrument**

**Exercise 4.3: IV regression**

**Exercise 4.4: Effects of diseases, overidentification tests & criticism**

**Exercise 5.1: 20 years later - introduction**

**Exercise 5.2: 20 years later - regressions**

**Exercise 6: Conclusion**

**Exercise 7 (optional): Data preparation**

**Exercise 8: References**

## Exercise 2.1: Data overview

### Importing the data

The first step of every analysis carried out in R or any other software environment is to load the necessary data. In our case, we will replicate a major part of the paper and we will use the data provided by the authors. All data is loaded from <a href="https://economics.mit.edu/faculty/acemoglu/data/ajr2001" target="_blank">here</a>, and the authors divided the dataset into 8 different data frames containing information to be used to get 8 respective tables that can be seen in the paper. In this exercise, we will load all data and prepare one dataset in a convenient form suitable for the most of following tasks and exercises.

Although many problem sets have pre-processed datasets, I have decided to intentionally leave this stage of data preparation in the exercise as this is a very important part of any research which often requires substantial time and effort, especially if the data is not readily available. Our case is still simplified, however, it gives a glimpse of this process.

We are going to import macroeconomic, geographic, and statistical data on 163 countries mostly from the period of 1980-1995, but some variables will even contain information from the year 1900.

*Note: : if you would like to first get additional details on how to prepare and clean data, please go to the optional **Exercise 7**.*

Now let's start with the first exercise.

We are going to import the necessary data here. Importing data into R can be done by simply assigning the imported data to a variable that we create by giving it a name. Generally, we write something like `new variable = read.table("data_name")` or `new variable = readRDS("data_name.rds")`.

In this task you should simply insert a name of the file `data.RDS`. Use the command `readRDS()` with `data.RDS` as an argument in quotes and assign it to the variable `data`.

```{r "2_1"}
# Use the command `readRDS()` with `data.RDS` as an argument in quotes and assign it to the variable `data`. Enter your code in the line below:

```



### First look at the data

Now let's have a look at what the authors provide in this table. Use the command `head()` with loaded table name as an argument to reveal the first few rows of the dataset.

```{r "2_2",optional=TRUE}
#Let's have a look at the data with the head() command. Enter your code here:

```

We have quite a lot of variables (25, to be precise), but many of them will be required only in the last exercises of this problem set.

As discussed earlier, we have some macroeconomic, geographic, and statistical data for different countries, and the two most important variables for our analysis will be `exprop` and `log_gdp_95`.

`exprop` measures the risk of expropriation of private foreign investment by the government with a scale from 0 to 10, where a higher score means less risk. Mean value for the scores in all years from 1985 to 1995 is reported.

 - `exprop` variable serves as a proxy for the quality of institutions and property rights. That being said, in response to expropriatory threats of one kind or another, entrepreneurs not only reduce investment, they also invest in less specialized capital (human and physical), which can be moved more easily from one activity to another. This has static efficiency effects but also discourages dynamic gains from innovation since innovation is most likely to thrive when specialization is encouraged.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("How risk of expropriation was calculated in the original paper?")
```

- `log_gdp_95` is a logarithm of Gross Domestic Product per capita in the year 1995, Purchasing Power Parity basis. This variable serves as an indication of economic performance and will be our main dependent variable in the further analysis.

**Since the paper was written 20 years ago, it would make sense not only to replicate the paper but also to check if the original conclusions hold when we use modern data. In order to do this, we will also include a new measure of expropriation risk and log GDP per capita in our dataset.**

**We will analyze the differences between the original paper and modern estimates in the section “Do the results hold 20 years later?”. More details on the modern variables and approach can also be found in that section.**

Here are some of the other important variables used in the replication exercise:

- `country`: ISO code of the country

- `base`: Dummy variable, which is equal to 1 if the country is included in the base sample, and 0 otherwise. Later on, the base sample will be predominantly used to carry out the analysis. The base sample was constructed by the authors with respect to the availability of data for countries and some other criteria that we will cover later.

- `log_mort`: Logarithm of estimated settler mortality during the colonization period. It is measured in terms of deaths per annum per 1,000 “mean strength” (raw mortality numbers are adjusted to what they would be if a force of 1,000 living people were kept in place for a whole year, e.g., this number can exceed 1,000 in episodes of extreme mortality as those who die are replaced with new arrivals).

- `latit`: Absolute value of the latitude of the country, scaled to take values between 0 and 1, where 0 is the equator. Latitude is the measurement of distance north or south of the Equator. It is measured with imaginary lines going parallel to the Equator.

- `africa`, `asia`, `other`: Dummy variables for the continents, where the country is located. Since it is clearly not the most visual way to present data, we have one column with names of continents for each country that we will use for descriptive analysis. Nevertheless in regression analysis later we will get back to the original dummies.

A more detailed description of other variables is given in the info block below. It could be useful to quickly skim the definitions, but there is no need to do it right now. You can always come back later and go through each definition in detail when we will use these variables in the further analysis.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Other variables in the table")
```

One of the most well-known commands for showing basic descriptive statistics is `summary()`. However, the output might appear not in the most convenient for further analysis form, especially if there are many variables in the dataset. This is because `summary()` yields an individual column for each variable.

The good news is that there are already some packages that make more visualized and clear summaries, like `stargazer` or `skimr`. We will use the `stargazer` package later to analyze the results of regressions, but now let's have a look at the command `skim` from the package `skimr`.

In this task you should first use the command `library()` to access the package `skimr`. You simply have to put the name of the package in brackets of the command without any quotes or special characters.

```{r "2_3"}
#First of all, load the necessary package `skimr`:
library(___)

#Now use the function `skim()` with our data as an argument. Enter the code below:

```

Great! Now we have a much nicer format. And it's time for some quizzes.


Quiz: How many countries do we have in a dataset?

[1]: 30
[2]: 163
[3]: 99
[4]: 29

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Counting countries")
```


Quiz: What percentage of the total dataset is our base sample?

[1]: 39%
[2]: 49%
[3]: 64%
[4]: Impossible to calculate

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Counting countries 2")
```

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Mathematics in R")
```

In order to solve the next quiz, you will probably need the space to operate with code. You can use the blank chunk below for calculations, if necessary.

```{r "2_5",optional=TRUE}

```


Quiz: What is the average GPD per capita in 1995 in the dataset?

[1]: 0.908
[2]: 4023.9
[3]: 8.3
[4]: 8300

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Trap 1")
```

## Exercise 2.2: Plotting the data
 
### Plotting the data - GDP

Ok, enough work with numbers for now. You probably noticed small histograms in the very right column. They show the frequency distribution of data points for each variable, which could be useful to get a quick understanding, but it is still not very informative.

Let’s plot some histograms on a bigger scale. One of the nice ways for plotting the data is to use the package `ggplot2`. In the task below we will first plot the histogram step by step in order to introduce the mechanics of the code in detail, later on, all operations will be performed in one code chunk.

a) First, we need load the `ggplot2` package. Then we should create a background, i.e. tell the function where to take data points from and what axes to construct. For this, we use the command `ggplot()`. Use it to assign the background to the variable `histogram_1`. We want to use `data` as a first argument and `aes(x = ..., y = ...)` as a second one to name the variables which we want to plot on X and Y axes. In our case we want to have a histogram-type of the chart in the end, therefore we only specify the x-axis and do not need to state the y-axis explicitly (i.e. we omit it in the code).

```{r "3_a"}

#First we load the package and the data
data = readRDS("data.RDS")
library(ggplot2)

#Assign the background to the variable histogram_1a. We want to plot the variable `log_gdp_95` on the x-axis.
histogram_1a = ggplot(data, aes(x = ___))

#Now call `histogram_1a` to see the background that we created.
histogram_1a
```

Fine, we can see that the background is ready.

b) Now we can add functions to the background by using the "+" sign. We are going to add a histogram itself to the background with the command `geom_histogram()` specifying the desired inner colour with `fill="orange"`, the border colour with `color="black"` and the width of histogram bins with `binwidth = 0.5`. The command `binwidth = ...` helps to get a better picture of the data and how it is distributed by adjusting the width of each category bar. It is important to choose a good value to have a balance between granularity and generality.

```{r "3_b"}
#Add histogram to the variable histogram_2 and reassign the whole plot to the variable histogram_2. Specify orange fill and black border colour. Use binwidth=0.5 in order to have a more general picture.

histogram_1ab = histogram_1a + geom_histogram(fill="___", colour="___", binwidth=___)
histogram_1ab
```

Perfect, we can already see the distribution of the data. But let us finish wuth the formatting before analyzing the histogram.

c) Now with the command `geom_vline()` we add red vertical line indicating the mean of the variable `log_gdp_95` (with omitted NAs) and with the command `linetype = "dashed"` change the line type to dashed to see it better. We again reassign the sum of previous `histogram_1` and new element to `histogram_1`.

```{r "3_c"}
# 2.) Add the vertical line showing the mean of `log_gdp_95` with omitted NAs and reassign the whole plot to the variable histogram_1. Specify size = 1, red colour and dashed linetype.
histogram_1abc = histogram_1ab + geom_vline(xintercept=___(data$log_gdp_95, na.rm=TRUE), size = 1, colour = "red", linetype = "___")
histogram_1abc
```

d) Last but not least we are going to add the text on the title, x- and y-axes of the graph by adding `+ labs(title="...", x = "...", y = "...")` and format text elements to be in a center of the chart. The title is "Log GDP Distribution in 1995", the x-axis name is "Log PPP GDP 1995", the y-axis name is "Count". We can also center the title with the command `theme(plot.title = element_text(hjust = 0.5)`, where `0.5` states the position of the title.

Then we again reassign the sum of `histogram_1` and new element to `histogram_1`.

```{r "3_d",fig.height=7, fig.width=8}
# 3.) Replace all ___ to add a title and axes names, then reassign everything to `histogram_2`. Set title to "Log GDP Distribution in 1995", x-axis name to "Log PPP GDP 1995", y-axis name to "Count".
histogram_1 = histogram_1abc + ___(title="___", x="___", y = "___") + theme(plot.title = element_text(hjust = 0.5))
histogram_1
```

Now let's have a look at the chart. 


Quiz: What could one say about the distribution of PPP GDG per capita based on the histogram we see above?

[1]: The sample is adequately balanced with most countries having average income and only a few countries having low and high GDP levels.
[2]: There is a big cluster of countries with low income, a smaller group of countries having higher than average income and a few countries having extremely high income.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Its a trap 2")
```

An important feature in the depicted distribution is that we can see a “jump” around Log GDP of 10, which corresponds to approx. USD 22,000 GDP per capita in 1995. This likely reflects the group of developed countries with similar and higher GDP figures. Let us also plot the original GDP figures to investigate this issue further.

In order to be able to compare the histograms directly, we will plot them next to each other. For this, we will use the package `gridExtra`, which allows to specify the layout of several charts and plot them all at once.

```{r "3_d_2",fig.height=7, fig.width=8}
# Load the package 
library(gridExtra)

histogram_1_adj = histogram_1 + ylim(0, 45)
histogram_1_exp = ggplot(data, aes(x = exp(log_gdp_95))) + geom_histogram(fill="orange", colour="black", binwidth = 2000) + geom_vline(xintercept=mean(exp(data$log_gdp_95), na.rm=TRUE), size = 1, colour = "red", linetype = "dashed") + labs(title="GDP Distribution in 1995", x="PPP GDP 1995", y = "Count") + theme(plot.title = element_text(hjust = 0.5)) + ylim(0, 45)

#In the `grid.arrange()` command we simply list all necessary histograms and specify the number of rows or columns.
grid.arrange(histogram_1_adj, histogram_1_exp, ncol=2)

```

Great, now we can see that our hypothesis is confirmed - we can see that most countries have GDP of around 3,000 - 5,000 USD, and that there is a clear group of countries with GDP figures around 18,000 - 25,000 USD, reflecting developed ones.



### Plotting the data - Quality of institutions

Now let us have a look at the quality of institutions. Just follow the instructions and build a new histogram analogously to that one above by filling the necessary information in the gaps.

```{r "3_d_3"}

#Create the histogram with the original protection against expropriation risk (variable `exprop`) on the x-axis and add a vertical line showing the average `exprop` value:

histogram_3 = ggplot(data, aes(x = ___)) + ___(fill="darkmagenta", colour="black", binwidth=0.5) + ___(xintercept=mean(data$exprop, na.rm=TRUE), size = 1, colour = "red", linetype = "dashed") + labs(title="Protection against expropriation risk 1985-1995",x="Protection against expropriation risk", y = "Count") + theme(plot.title = element_text(hjust = 0.5))

#Now plot the histogram
histogram_3

```


On the chart we can clearly see that there are 2 peaks at around average and maximum scores, reflecting groups of developing and developed countries, with a lower number of countries that received extremely low or above-average ratings.

Overall, the graphical illustration of the variable suggests that the distribution of the quality of institutions is skewed to the right and that there are more countries that tend to have higher quality institutions with more property rights. This is quite normal due to the fact that “grades” for expropriation risk were assigned by people.


## Exercise 2.3: Descriptive analysis & bivariate relations

### Deep-dive into GDP differences: looking at different areas

While further exploring the data, we can perform a sanity check. Let's make an educated guess first.


Quiz: According to common knowledge, which continent has the lowest average/median GDP?

[1]: Africa
[2]: Asia
[3]: South America
[4]: Europe

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Common sense")
```

Let’s check this by extracting the median and average GDP values across countries in each continent.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Median vs. average values")
```

In order to show median and average across countries in each continent, we will use `group_by` and `summarise` commands as well as the pipe operator `%>%` from the `dplyr` package. If you are already familiar with them, feel free to start with the code chunk, otherwise it could be helpful to read the info block below:

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Commands `group_by()`, `summarise()` and `%>%`")
```

```{r "4_1"}
#As usual, first we need the respective package to make use of its commands.
data = readRDS("data.RDS")
library(dplyr)

#Now we want to use group_by() and summarize() commands. Your task is to group the data properly to confirm our guess in the quiz above, so replace the first ___ in the group_by() command with a correct variable. You should also add correct variables in the second and third spaces to make R calculate medians and averages of the absolute GDP figures.

data[!is.na(data$log_gdp_95),] %>% 
  group_by(___) %>% 
  summarise(Median_GDP_per_capita_95 = round(median(exp(___)), digits = 0), Average_GDP_per_capita_95 = round(mean(exp(___)), digits = 0))

```

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Is Oceania a continent?")
```

As you can see, the variation between countries on different continents is quite significant. Median income per capita across countries in Africa is less than 1500 USD , whereas it is significantly exceeding 10 000 USD in countries in Europe and Oceania.

Let us also visualize these differences, thus making it much easier to interpret them. We will use the pirateplot function.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("pirateplot")
```

Now it's time to plot it:

```{r "4_2"}
#First we load the necessary package
library(yarrr)

#We would like to show absolute GDP per capita figures on the y-axis and continents as a categorical variable on the x-axis:
pirateplot(formula = exp(___) ~ ___,
           data = data,
           theme = 1,
           pal = "basel",
           main = "1995 GDP per capita by continents",
           ylab = "GDP per capita 1995", 
           xlab = "Continent")
```

Now let's have a deeper look at the results.

-	We can see that countries in Europe and Oceania have the highest GDP per capita, which is not surprising. Moreover, in Europe, we can easily notice 2 groups of countries on the pirateplot chart, which reflect more economically developed Western European countries and less developed Eastern European ones.
- Oceania contains only 3 countries with recorded GDP, and if we take the median, it will show GDP of the New Zealand, which is why it is so high.
- As concerns countries in North America, the low average and median numbers might seem surprising at a first glance, but we should remember that although there are rich countries like Canada and the USA, all other countries are rather poor, which gives a low average and median values of countries’ GDP per capita. This can also be clearly seen on the pirateplot chart.

Later on, we will try to understand the fundamental causes of such a huge difference.

### Deep-dive into GDP differences: adding temperature

At this point, it would also be interesting to dive a little bit deeper and add average temperature (measured in °C). As usual, we start with a little quiz.


Quiz: Make a guess, what kind of relation could one see while examining average temperature, GDP and location of the country?

[1]: African countries have highest average temperature and lowest GDP
[2]: Asian countries have moderate average temperature and lowest GDP
[3]: African countries have highest average temperature and moderate GDP
[4]: No relation should be seen

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Common sense 2")
```

Now let’s check if our data confirms the hypothesis by plotting the necessary data.

Here we also introduce a nice package that helps to build interactive charts - package “plotly”.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("plotly")
```

Just run the code chunk below.

```{r "4_3",output='htmlwidget', widget='plotly'}
library(plotly)
p = data %>%
    ggplot(aes(x = log_gdp_95, y = meantemp, colour = continent)) + 
    geom_text(label=data$country) +
    scale_color_hue() + 
    labs(x="Log PPP GDP", y = "Average temperature in °C")
ggplotly(p)
```

You can hover the mouse over any data point to reveal additional information about the concrete observation. Additionally, you can click on each category on the legend to remove it from the chart (or double-click to show only this category).

Fine, we can clearly see that African countries are mostly plotted in the top left corner with the highest average temperature and lowest GDP, whereas Asian and other countries are taking middle and middle-right positions of the chart.

Note: the chart only shows countries for which authors have collected data and which were colonies in the past - this is our base sample. More details will be given in the section 2.4.


### Relation of GDP and expropriation risk

Alright, we have seen that the data meets the general requirements of adequacy, and let’s now get back to our main topic and check if we can find any dependencies between the quality of institutions and economic performance.

First, let’s simply plot the variables that we are interested in at most  - `log_gdp_95` and `exprop`. Before we do this, it is important to mention at this point that all future analyses will be based not on the initial sample, rather on the base one. First, we need to detach the base sample from the whole, and this can be easily done with the help of the command `filter`. By default it requires two arguments, the original dataset and a rule/condition to filter the data, e.g. `filter(data, data$x == "5")`

```{r "4_4"}
#Use the command `filter()` in order to get a base sample from the original dataset and save the new base data in the variable `data_base`. Second argument should define the value of a variable `base` to be equal to 1. It is important to use double equals sign `==` when you define a condition.
data = readRDS("data.RDS")
data_base = ___(data, data$___ == "___")
```

Great, now we have a base sample to work with. It is limited to the 64 countries that were ex-colonies and for which we have settler mortality, protection against expropriation risk, and GDP data. If you want, you can call the command `summary()` or `skim()` in a chunk below to have a quick overview here. This is not a necessary task, as the data will have the same structure as our original dataset.

```{r "4_5",optional=TRUE}
# Enter your code here.
```

Well, now it’s time to actually plot the variables. First, let’s try to anticipate what we could see on this chart.


Quiz: Would you expect to see any dependence or pattern on the chart at a first glance?

[1]: Yes, we expect to see a positive correlation between log GDP and protection against expropriation risk
[2]: Yes, we expect to see a negative correlation between log GDP and protection against expropriation risk
[3]: No, there will likely be no obvious correlation

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Following trends")
```

Now let us check our assumption by plotting expropriation risk and GDP values across the base sample.

```{r "4_6",output='htmlwidget', widget='plotly'}
#Replace ___ in order to construct a plot showing expropriation risk and gdp values across the base sample. Place expropriation risk on the x-axis, GDP on the y-axis, and colorize countries by continents.
library(plotly)
plot_1 = data_base %>%
    ggplot(aes(x = ___, y = ___, colour = ___)) +
    geom_text(label=data_base$country) + 
    labs(x="Average protection against expropriation risk 1995", y = "Log PPP GDP 1995")

#Here we use ggplotly() function to make the chart interactive:
ggplotly(plot_1)

```

Perfect, now let’s look at what we have on the chart. Here we can see that countries with higher protection against expropriation risk have higher GDP, and we can confirm the anticipated positive correlation. On the left chart, which replicates the result from the paper, the correlation is stronger. On the one hand, this might be because of more precise data used by the authors. On the other hand, it could also be the case that the association is getting weaker with time. We will investigate this issue further in the next chapters.

After we noticed the positive correlation on the chart, we certainly want to make it a bit more formal, namely, to display it on the chart. Luckily, `ggplot2` package allows us to do so.

Before we go on, let us remember some theory. We can give R a command to add a linear measure of this correlation on the chart, and R will draw a straight line by minimizing the difference between the observed data points and closest points on this drawn line. It is important to mention that squares of each difference are taken in order to penalize high errors, and then the sum of squared errors is minimized. This method is called `Ordinary Least Squares`, or just `OLS`, and you will find more detailed information about it in the next chapter.

Alright, let us give R a task to make this work for us and add a line on the chart. Note that we use the chart from the previous task without grouping the observations by continents and also modify the code by adding one more element.

```{r "4_7",output='htmlwidget', widget='plotly'}

plot_1_line = data_base %>%
    ggplot(aes(x = exprop, y = log_gdp_95)) +
    geom_text(label=data_base$country) + 
    labs(x="Average protection against expropriation risk", y = "Log PPP GDP")+ 
    geom_smooth(method='lm', se = FALSE)
ggplotly(plot_1_line)

```

Perfect, now we can see that the linear model only confirms our thoughts - there is indeed a strong correlation between our measure of institutions and income per capita. The line on the chart captures this relation between the data points and has a positive slope.

At this point, we will stop simply exploring the available data and blindly looking for all kinds of patterns. In the next exercise, we will learn more about why we are so specifically interested in institutions, delve deeper into the analysis of the relationship between variables with the help of the regressions.



## Exercise 3.1: OLS regression

In this exercise, we will build a simple linear model, analyze the correlations between the variables in more detail and think about how we can interpret the results of the regression.

**_________________________________________________________________________________________________________________________________**

**Disclaimer:**

For illustrative purposes, simplicity, and in order to avoid impreciseness of formulations, in the following exercises, we first assume that the linear OLS model can adequately capture the relationship between variables and there are no endogeneity problems impacting the causal relationship. The following exercises are presented in a way to gradually reveal potential obstacles/problems and find solutions, therefore getting to more complicated models step by step.

**_________________________________________________________________________________________________________________________________**

Let us first remember, why are we interested in institutions as a whole. There are many articles and papers that explore the impact of institutions on economic performance, and the general conclusion is that level of income is dependent on the amount of investment in physical and human capital, which is in turn strongly dependent on the security of property rights and development of institutions in that country (North, 1981, Jones, 1981).

This view was also confirmed by cross-country correlations between measures of property rights and economic development (e.g., Knack and Keefer, 1995, Mauro, 1995, Hall and Jones, 1999, Rodrik, 1999), and by micro-studies that explore the relationship between property rights and investment or output (e.g., Besley, 1995, Mazingo, 1999, Johnson, McMillan and Woodruff, 1999).

Authors of our article provide some empirical examples that almost everybody is aware of - they compare paths of North and South Korea, or East and West Germany, where differences in economic performance and development started to grow drastically shortly after they chose opposite paths - central planning and collective ownership versus market economy and private property.

### Ordinary Least Squares (OLS)

Let us remember the last chart from the previous exercise where we also saw quite an obvious correlation between institutions (proxied by expropriation risk) and now try to explore it deeper with the help of an OLS method, which we discussed earlier. This time, however, we will construct the regression ourselves and analyze the output.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Assumptions of the OLS model")
```

We start with the basic linear model and the following formula: 

$$Log\_GDP\_95_{i}=\beta_{0}+\beta_{1} \cdot exprop_{i}+u_{i}$$
where $Log\_GDP\_95$ is the income per capita in country $i$, $exprop_i$ is the protection against expropriation measure, and $u_{i}$ is a random error term. The coefficient of interest throughout the paper is $\beta_{1}$, with which we aim to measure the causal effects of institutions on income per capita.

Now let us run the regression with the command `lm()` which can be used for linear models. In the basic form command `lm()` requires dependent and explanatory variable linked with `~` sigma as first argument and `data = ...` as a second one. There is no need to specify constant as lm automatically includes it by default. Let us call this command to build the regression similar to the formula above.

```{r "5_1"}
#First in the beginning of each exercise, we load the data again in order to make it possible to solve exercises in any order.
data_base = readRDS("data_base.RDS")

#Now call the command 'lm()" in the following format: lm(dependent variable ~ explanatory variable, data = ...). We save the regression under the name `reg_1_1`:

reg_1_1 = ___(___ ~ ___, data = data_base)
reg_1_1
```

Great, the regression is built. However, the current output is quite limited, as we can only see the estimated coefficients.

We can call a `summary()` command to obtain a more detailed output, but we will use a better-looking version from the `stargazer` package. Command `stargazer()`allows us to create nice tables as they are printed in articles with many columns from different regressions, and to directly compare the results.

Load the package `stargazer` and call the command `stargazer()` with regression name as a first argument and `type = "html"` as a second one. 

```{r "5_2",results='asis'}
#Load the package `stargazer` and replace both ___ in the command `stargazer()` with 2 regression names as a first and second arguments and `type = "html"` as a third one:
library(stargazer)
stargazer(___, ___, type = "html")
```

Numbers in columns indicate the estimates of the coefficients, and their standard errors are in parentheses. This format is standard practice for recording the results of econometric simulations, as it allows the reader to gauge the accuracy of the results and estimate the confidence intervals for the coefficients. At the bottom of the table, among other indicators, you can find the R-squared coefficient.

If we wanted to write the estimated regression equation, we would then record it as follows:

$$\widehat{Log\_GDP\_95_{i}}=4.660+0.522 \cdot exprop_{i}$$

Alright, let us now examine the meaning of these estimates.


Quiz: Assuming that all assumptions of the OLS model are fulfilled and looking at the estimates of the coefficients, can we always interpret any number in the table in terms of its impact on the dependent variable?

[1]: Yes, the true value of the dependent variable should change accordingly to the estimated coefficient given the change of explanatory variable
[2]: No, we should first check whether the actual coefficient is not zero

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Its a trap 3")
```

Well done, not all coefficients can always be interpreted. This is because $\beta_1$ and $\beta_2$ are are only estimates obtained by using OLS on the basis of a given random sample. Consequently, they are random variables that can take on values that are only “approximately” equal to the true ones. Therefore, even if the true value of the coefficient is zero, its estimate is likely to deviate from zero. Therefore, it is necessary to be able to determine whether the coefficient differs strongly enough from zero, so that one can confidently assert that the true value of the coefficient is also not equal to zero.

### Statistical significance

In the info block below you will find a detailed procedure for testing the insignificance of the coefficient.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Procedure to test for insignificance of the coefficient")
```


Quiz: Do we always need to follow this complicated process in order to decide on the significance of the coefficients?

[1]: Unfortunately, yes
[2]: No, there must be a shortcut

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Shortcut")
```

Luckily, there is an alternative way to do so. We can use the so-called p-value to test the hypothesis.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("p-value")
```

In the summary table we can see small stars near each of the coefficients, which are basically the indication of p-values. It is also written in a note, where 1 star $(*)$ means that p-value is less than $0.1$, 2 stars $(**)$ mean that p-value is less than $0.05$, and 3 stars $(***)$ means that p-value is less than $0.01$. As discussed in the info block above, the lower the p-value, the more we can be sure that the coefficient is actually significant.


Quiz: Are coefficients in our regressions significant?

[1]: Both are insignificant
[2]: Only `exprop` is significant
[3]: Only `constant` is significant
[4]: Both are significant

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Significant significance")
```

### R-squared

Let’s now get back to our regression summary. We can also see the R2 value, which is equal to 0.540 in the regression.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("R-squared, a.k.a. coefficient of determination")
```

In our summary, the R-squared of the regression indicates that approx. 50 percent of the variation in income per capita is associated with variation in expropriation risk, which is already quite a lot taking into account that we have only included one explanatory variable while trying to predict such a sophisticated thing like GDP.

### Interpretation of the coefficients

Well done! Now it's finally time to interpret the coefficients. 

In our case, we could say that if the quality of institutions exprop is increased by one unit, then we predict the logarithm of the income per capita `Log_gdp_95` to increase by 0.52, but this is not very informative. We would like to see the actual relation between the income and quality of institutions, not logarithms, which brings us to another important point in the interpretation of the coefficients. If you are already familiar with different types of models, just skip the info block and start with the next quiz, otherwise, I recommend reading it.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("How to interpret variables in models with logarithms?")
```

Now let's interpret the coefficient in the informative way. You can use the chunk for calculations, if necessary.

```{r "5_3",optional=TRUE}

```


Quiz: Based on the 1995 regression, how should income per capita change (exactly) if `exprop` increases by one?

[1]: It is predicted to increase by 0.522
[2]: It is predicted to increase by 52.2%
[3]: It is predicted to increase by 0.685
[4]: It is predicted to increase by 68.5%

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Interpretation 1")
```



### Magnitude of the effect of institutions

At this point, we have had a look at the variables and their relation, but the magnitude of the effect of institutions on GDP is still not intuitively clear. We have seen that different countries have different scores, but we have not seen a practical example of associated differences in GDP.

In order to fill this gap, we will use the same approach as the authors and will take 2 countries from the sample. We will take those that stand in approximately 25th and 75th percentiles of the institutional measure in this sample. This approach will help us to interpret variables in a more intuitive way as well as to assess the magnitude of the analyzed effect.

```{r "5_4"}
#We would like to select only the necessary columns from the dataset (country code, expropriation risk and logarithm of its 1995 GDP) and then get one value on the basis of 25th percentile of `exprop`. For this we will use select function with a very simple syntax, reflecting a vector of columns to be subset.
Q25 = data_base %>%
      select(c(country, exprop, log_gdp_95)) %>%
      filter(exprop > quantile(exprop, 0.25) & exprop < quantile(exprop, 0.26))
Q25
```

We have found a country that stands in the ~25th percentile. It is Bolivia with `exprop` rating of 5.64 and `log_gdp_95` of 7.93.

Now let's also find a country in the ~75th percentile:
```{r "5_5"}
#Subset country code, expropriation risk and logarithm of its GDP on the basis of 75th percentile of `exprop`.
Q75 = data_base %>%
      select(c(country, exprop, log_gdp_95)) %>% 
      filter(exprop > quantile(exprop, 0.74) & exprop < quantile(exprop, 0.75))
Q75
```

It is Colombia with an `exprop` rating of 7.32 and `log_gdp_95` of 8.81.

```{r "5_6"}

#We will also bind both outputs with the help of the command rbind(), which can be used to combine several vectors, matrices and/or data frames by rows. Fill in the ___ with the vectors from the previous chunks.
rbind(___, ___)

```

So the actual difference in income between these countries is $8.81 - 7.93 = 0.88$ log-points, or $(e^{0.88}-1) \cdot 100\%=141\%$.
Predicted by the model is $0.522 \cdot (7.32 - 5.64) = 0.88$ or also $(e^{0.88}-1) \cdot 100\%=141\%$.

As we have just seen, the estimated effect implies a strong effect of institutions on performance and even seems to match the actual picture we see in the sample. Remember that we also saw that the coefficients are significant and R-squared is high enough.


Quiz: Does this example mean that the model would always precisely predict the growth of GDP based on change of expropriation risk?

[1]: Yes, since the predicted percentage change exactly matches the observed one
[2]: No, since it is likely that we were lucky with the chosen figures this time

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Its a trap 4")
```



## Exercise 3.2: Bias, endogeneity & control variables

In this exercise, we will consider more advanced points like omitted variable bias and endogeneity problems. Moreover, we will check whether we can interpret the results of the simple OLS model in a causal way.

### Facing the bias

**We have seen that the regression can sometimes precisely predict the change in GDP based on the change in expropriation risk rating. But does this method help us to derive an unbiased estimator of the causal effect of quality of institutions on GDP?**

**The answer is no. The first reason is that there should be other variables that impact the GDP, which leads to biased estimates.**

Let us remember the form of the regression equation that we estimated with the OLS method:

$$Log\ GDP\ 1995_{i}=\beta_{0}+\beta_{1} \cdot Protection\ against\ expropriation\ risk_{i} + u_{i}$$

*Note: instead of `log_gdp_95` and `exprop` variables we write full names of the variables in this equation to make the idea more clear. In the R environment we still stick to the proper short names of the variables*

The first problem that arises here is related to the explanatory variables that we use. Apart from the random error, which is always included in every model, we try to explain the variation in the level of income only by using the quality of institutions in the regression equation. But does it actually help to find the answer and reveal the causal relation?

What if there are some other factors that influence both the quality of institutions and the level of income? One such factor could be geography, which likely had a direct impact on both quality of institutions and the level of income. For example, geography might define access to natural resources, which have an impact on GDP, and the country could have developed neighbors and improved institutions through relations with them.

However, geography is certainly a very broad term, and we need to find some more specific factors that we can use as a proxy. In our paper authors mention, that other social scientists like Montesquieu (1989), Diamond (1997) and Sachs and coauthors (1998) have argued for a direct effect of climate on performance, and Gallup, Mellinger, and Sachs (1998) and Hall and Jones (1999) found the correlation between distance from the equator (latitude) and economic performance. We will use latitude as a proxy for geography, as we also have latitude values in our dataset.

Given this logic, let us assume here that the “true” data generating process includes latitude and looks like that:

$$Log\ GDP\ 1995_{i}=\beta_{0}+\beta_{1} \cdot Protection\ against\ expropriation\ risk_{i} + \beta_{2} \cdot Latitiude_{i} + \epsilon_{i}$$

What then happens when we estimate the OLS regression without latitude like we did before?

![Comparison of short and long regressions](Long-short equation.png)

*Source: own illustration*

In our original OLS specification ("Short regression") with only one explanatory variable (Protection against expropriation risk, variable `exprop`) used in the regression, the unobserved error $u_i$ was actually of the form $\beta_{2} \cdot Latitude_{i} + \epsilon_{i}$, but other variables (Latitude) were considered all together mistakenly as random errors in $u_i$, were actually impacting $Protection\ against\ expropriation\ risk_i$ and biasing its estimator.

If this is the case, a so-called omitted variable bias arises. This violates the exogeneity assumption of the OLS (return to the info block covering the OLS assumptions, if necessary). Therefore, we cannot make any relevant conclusions if the effect on GDP is exclusively because of the $Protection\ against\ exproprtiation\ risk_i$ or if there are others variables that could actually influence $Protection\ against\ exproprtiation\ risk_i$ to be significant in explaining $Log\ GDP\ 1995_i$.

Graphically this can be shown in the following way:
![Comparison of short and long regressions](Long-short DAG.png)

*Source: own illustration*

Such situation depicts a problem known as **endogeneity**. If the explanatory variable is endogenous, then the results of modeling cannot be interpreted in terms of causation. 

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Exogeneous and endogeneous explanatory variables")
```

We will dive into the general endogeneity problems deeper a bit later, but now let us focus on the bias first.

We know that the bias is likely present in the OLS model, so we could also predict the direction of bias that occurs when the model omits a confounding variable like latitude by analyzing the relation between the variables.

In the picture above there are (+) signs next to arrows that show positive type of the relation between the variables based on the following:
- Better protection against expropriation risk is associated with higher GDP
- Higher latitude means that the country lies farther from the equator, which should be associated with higher GDP (since African countries are closer to equator than European countries / Australia / Canada). 
- Same logic applies to the protection against expropriation risk, however, it is not that obvious. 

Estimating correlations can help to determine the direction of the relation (and even the direction of the bias itself). More details could be found in the info block below.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Correlation structures as an approach to determine the direction of the bias")
```

As stated in the info block above, correlations might help to confirm our hypothesis about the relation of the variables.
```{r "6_1"}

data_base = readRDS("data_base.RDS")

#Replace the ___ with the correct variables to calculate the correlation between latitude and protection against expropriation risk as well as between latitude and log GDP 1995.

cor(data_base$latit, data_base$___)
cor(data_base$___, data_base$log_gdp_95)


```



Quiz: Which direction of bias could we expect, if we assume that the omitted variable is the only source of the bias? What would be the impact on the coefficient?

[1]: Positive, which means that the OLS coefficient should be underestimated
[2]: Positive, which means that the OLS coefficient should be overestimated
[3]: Negative, which means that the OLS coefficient should be underestimated
[4]: Negative, which means that the OLS coefficient should be overestimated

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Bias")
```

The explanatory variable `Protection against expropriation risk` and the omitted confounder `Latitude` have a positive relationship. The omitted confounder `Latitude` and the dependent variable `Log GDP 1995` also have a positive relationship, which indicates that we can expect a positive bias, i.e. the coefficient in the OLS model that we calculated in the previous exercise ($0.522$), should be overestimated.

Let us check this assumption:
```{r "6_2",results='asis'}

#Since this is a new erxercise, we have to replicate the reg_1_1 first:
reg_1_1 = lm(log_gdp_95 ~ exprop, data = data_base)

#Now replace ___ in the code to run a regression with latitude. We want to use base sample.
reg_1_2 = lm(log_gdp_95 ~ ___ + ___, data = data_base)

#Show summary of these regressions with the help of the command `stargazer()` by just adding all regression names as arguments.
stargazer(___, ___, type = "html")

```

We can see that the assumption about the direction of bias was correct, as we see that adding `Latitude` decreases the coefficient from $0.522$ to $0.468$ (talking about predicted $\%$ change in GDP per capita when `exprop` increases by $1$, it decreases from $68.5 \%$ estimated in the "Short regression" to $59.7 \%$ in the "Long regression").

Such variables that we add to the model (like `Latitude` in our case) to eliminate the bias in the estimate of the coefficient of interest are called **control variables**. 

- A variable of interest is a factor whose influence on the dependent variable we are interested in (i.e. `exprop`)
- Control variables (e.g. `latitude`) are variables that we include in the model in order to avoid bias in the coefficient for the variable of interest and associated endogeneity problems.

In order to avoid this bias and potential endogeneity problems, it is necessary to take into account all significant factors in your regression, which almost always makes us analyze the multiple regression model with control variables.

Let us first suppose that we have the missing variable data. Then, as you have probably already guessed, we just need to add the missing variable to the model to solve the problem.

In practice, there are often several missing variables (since the world is complex and the dependent variable is usually influenced by many factors at once). Well, then you need to add them all.

But what variables should we add to the model first? As discussed earlier, geography, as the location of the country has a huge impact on soil quality, amount of resources, length of the day and night, temperature, and climate. It can even be the case that the entire correlation is spurious, i.e. that there is actually no causal relationship between economic development and institutions. For example, suppose that there is no direct link between economic development and institutions, and the observed correlation is driven by a third factor like geography.

Let us include latitude and continent dummies in the regression equations so that they will now look like this:

$$Log\ GDP\ 1995_{i}=\beta_{0}+\beta_{1} \cdot Protection\ against\ expropriation\ risk_{i} + X'_{i} \cdot \gamma+\nu_{i}$$

where $X_i$ is a vector of added covariates.

Below you will find some additional information about the usage of dummy variables in the
regression and their interpretation.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Dummy variables in the regression")
```

Let us now build respective regressions and have a look at the results:
```{r "6_3",results='asis'}
#Add latitude and continent dummies for Asia and Africa.
reg_1_3 = lm(log_gdp_95 ~ exprop + ___ + ___ + ___ + other, data = data_base)

#Show summary of all 3 regressions including those from the previous task:
stargazer(___, ___, ___, type = "html")


```

In the `reg_1_3` we have included dummy variables `asia`, `africa` and `other`. However, there are countries in the dataset like Argentina that are not in Asia, Africa or `other` group. This is because there is one implicit group that includes countries located in America, but there is no separate column in the dataset as well as we do not include this group in the regressions.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Why don't we simply add the dummy for America?")
```

Let us now look at the results of the regressions.


Quiz: Which regression specification (1, 2 or 3) better fits the observed data?

[1]: 1
[2]: 2
[3]: 3
[4]: Cannot say

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Choosing favourites")
```

For the next quiz you might want to calculate some numbers, you can use the chunk below for calculations, if necessary.

```{r "6_4",optional=TRUE}

```


Quiz: How can we interpret coefficient if front of `africa` variable in the regression (3)? Use exact calculations.

[1]: All other factors fixed, income per capita in African countries is predicted to be on average 88.1% lower than in all other countries
[2]: All other factors fixed,, income per capita in African countries is predicted to be on average 58.6% lower than in all other countries
[3]: All other factors fixed, income per capita in African countries is predicted to be on average 88.1% lower than in American countries
[4]: All other factors fixed, income per capita in African countries is predicted to be on average 58.6% lower than in American countries

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Interpretation 2")
```

As we have seen, control variables like latitude or dummies continents actually have some portion of the influence that was previously attributed to expropriation risk. Therefore, we see that the coefficient in front of `exprop` decreases from  $0.522$ in the first model to $0.401$ in the third one. Notice that latitude is statistically significant only in the equation $(2)$, while not all continent dummies are significant in the $(3)$.

**In most further regressions we will add latitude as a control variable to account for the potential impact of geography. Authors provide regressions both with and without latitude for each case, but we will not do so to save time and space. This will not influence our conclusions and we will not miss anything important though.**


Quiz: Fundamentally, will including more control variables improve the quality of causal inference in our model?

[1]: Yes, including several control variables has already helped. Let's think of more of them!
[2]: No, we will unlikely be able to include all necessary control variables and need another approach to overcome fundamental endogeneity problems.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("To be on the right track")
```

Although it is true that there could be lots of other factors not represented in our model such as predominant religion, colonizer country, or ethnolinguistic diversity which could be highly correlated with quality of institutions and impact economic performance at the same time, we should not start with extending the model with all of these variables.

**Although results of the regressions with control variables show a strong correlation between the quality of institutions and economic performance, we should still not interpret this relationship as causal, as there is a number of reasons to consider the model endogenous.**

- Firstly, there could be many omitted determinants of income differences that will naturally be correlated with institutions, and it could be extremely difficult to take into account all of them with control variables.
- Secondly, there might occur a reverse causality problem. We use `exprop` as a measure of the quality of current institutions, but perhaps it is not that countries with better institutions are getting richer, but rather that rich countries can afford better institutions.
- Thirdly, there could be a lot of measurement errors. Indeed, it is unlikely that the index used (protection against expropriation risk) can ideally characterize such a complex concept as the quality of institutions and protection of property rights. Moreover, this rating of expropriation risk was constructed ex-post, and analysts, who are usual humans, may have had a natural bias in seeing better institutions in places they thought should have better institutions or just in richer ones.

This is the end of this set of exercises. In the next set of exercises, we will continue to explore endogeneity problems and will try to find a way how to deal with them.


## Exercise 4.1: Instruments against endogeneity

In this exercise, we will delve a bit deeper into endogeneity problems and ways to overcome them.

Let us introduce one feature that will also help to better understand the logic of our steps, which will become slightly more complicated.

We will visualize the model and all the next steps with the help of a Directed Acyclic Graph (DAG). For this, we can load the necessary packages `ggdag` and `dagitty`.

*We will not deep dive into the way how DAGs can be constructed in R, but if you want to know more, you can always check it at https://cran.r-project.org/web/packages/dagitty/index.html and https://cran.r-project.org/web/packages/ggdag/index.html.*

**_________________________________________________________________________________________________________________________________**

**Disclaimer:**

- From this moment on, we will use DAGs mostly for illustrative purposes to visually map the relation between variables and to understand the process more intuitively.

- We keep using the same variable for random error on the DAGs although the composition of the error actually differs. Regressions that we will analyze reflect only part of these DAGs and are not always directly related to the picture.

- Nodes are therefore not called as variables used in the regressions, but rather reflect the idea of the model.

- The regression equations in the text blocks are also presented for illustrative purposes in order to demonstrate the logical structure and therefore do not include exact names of variables.

**_________________________________________________________________________________________________________________________________**

Let us now visualize our situation in order to understand better, how the endogeneity arises although we have included control variables in the model.

**This DAG will look different from the DAGs we used in the previous exercise. This is a necessary measure to cover the next steps in an intuitive way. All following DAGs will have a similar look to the one we build now.**

```{r "7_1"}
#First we load the necessary packages:
library(ggdag)
library(dagitty)

#Set a good-looking visual theme:
theme_set(theme_dag())

#Code a DAG internal structrue:
dag_1 <- dagitty('dag {   I <-> Y <- v
                         v -> I <- X -> Y
  I [pos="0,0"]
  v [pos="0,1"]
  Y [o, pos="1,0"]
  X [pos="0.5,-0.5"]
   }') %>% 
  dag_label(
    labels = c(
      "I" = "Quality of Institutions", 
      "Y" = "Level of Income", 
      "v" = "Error",
      "X" = "Geography"))

#Plot the DAG itself:
ggdag(dag_1, use_labels = "label")

```



Although the look is somewhat different, on this DAG we can clearly see the situation as well: Error term $u$ influences both quality of institutions $I$ and level of income $Y$, and there are some other confounding factors like geography that also impact $I$ and $Y$.

Earlier we have already discussed the omitted variable bias and explained how explanatory variable may be correlated with the error term, and even tried to solve this problem. Indeed, the first problem described above can be partially solved by including a lot of control variables in the model. However, we can only overcome the other two difficulties by applying a specific method, which is called **instrumental variable estimation**.

As was mentioned earlier, our base sample includes countries that were once colonized by European countries. In this setting we dispose a number of economies with relatively similar income levels 400 years ago but where we observe large differences in per capita income in 1995, and authors of the paper state that colonization experience played a significant role in shaping institutions in these countries.

In order to understand the relationship better, we will use backward induction to finally arrive at the exogenous factor that could have defined the way we see institutions in these countries now. In other words, all we need is to find some factor that can be a source of exogenous variation, called an **instrument**.

Moreover, the instrument that we will use should satisfy two important conditions:
- **relevance**: $Cov(exprop_i, instrument_i)\neq 0$;
- **exogeneity**: $Cov(instrument_i, u_i) = 0$.
*Source: Wooldridge (2018), ch. 15*

The first property says that this variable should be correlated with our endogenous regressor. The second property requires that this variable should not be correlated with the random error of the model.

Graphically, the relationship between variables is shown in the figure below:

![Note: uncrossed arrows indicate correlation, crossed out arrows indicate no correlation.](Instrument.png)

*Source: own illustration*

## Exercise 4.2: Choice of the plausible instrument

In this exercise, we would like to find any factor that can be a source of exogenous variation in institutions in order to overcome endogeneity problems later.

We will use backward induction step-by-step to finally arrive at the exogenous factor that can be a source of exogenous variation in quality of institutions.

### Persistence of institutions

First of all, it is logical to assume that current institutions are very strongly related to past institutions, i.e. that they tend to persist for quite a long time.

- To start with, it is costly to set up functioning institutions, which place restrictions on government power and protect property rights (Acemoglu and Verdier, 1998). If the money is already spent, it is often not in the interest of the elites to switch from this set of institutions to extractive institutions, but it is rather more rational to exploit the present system. In case that extractive policy was applied earlier, the new elites will also less likely want to incur the costs of introducing better institutions, and may instead prefer to exploit the existing extractive institutions for their own benefits.

- Secondly, the size of the ruling elite may impact gains from the extractive strategy. This can be well illustrated with a cake and number and size of pieces that it could be cut in. The smaller the ruling elite is, the greater incentive it has to continue with the extractive policy (Acemoglu and Robinson, 2000). Authors of the paper state that in many cases where European powers set up authoritarian institutions, they delegated the day-to-day running of the state to a small domestic elite, which favored extractive institutions even after the independence.

- Finally, Acemoglu (1995) emphasizes that agents will be more likely to support institutions in case they made complementary irreversible investments, e.g. those who invested a lot in human and physical capital will be more likely eager to spend additional money to enforce property rights, while those who have less to lose will not be.

Therefore, we can expand our DAG to include this feature.

```{r "8_1"}
dag_2 <- dagitty('dag {   C -> I <-> Y <- v
                         v -> I <- X -> Y
  I [pos="0,0"]
  v [pos="0,1"]
  Y [o, pos="1,0"]
  X [pos="0.5,-0.5"]
  C [pos="-0.5,0.1"]
   }') %>% 
  dag_label(
    labels = c(
      "I" = "Quality of Institutions", 
      "Y" = "Level of Income", 
      "v" = "Error",
      "X" = "Geography",
      "C" = "Early institutions"))

ggdag(dag_2, use_labels = "label")
```

What we are interested in is the relation between early institutions and quality of institutions. Now let us build a regression in order to check whether we could confirm correlation of past and current institutions.

The regression equation should have the following logical structure: $$Quality\ of\ institutions_{i}=\beta_{0}+\beta_{1} \cdot Early\ institutions_{i} + X'_{i} \cdot \gamma+\epsilon_{i}$$, where $X_i$ is a vector of other added covariates.

```{r "8_2"}
#Here we again first load the data and necessary package that we will use in the next chunks. 
data_base = readRDS("data_base.RDS")
library(stargazer)
```

In the next code chunk we will build the regressions with a measure of early institutions as an explanatory variable and quality of institutions (expropriation risk) as a dependent one. There is no certain approach to define “early institutions”, so authors proxy them in 2 ways: constraints faced by the executive in 1900 and democracy index in 1900. In all specifications we also add the latitude variable in order to control for possible geographic factors.

```{r "8_3",results='asis'}
#Build regressions:
reg_2_1 = lm(exprop ~ cons_1900 + latit, data = data_base)
reg_2_2 = lm(exprop ~ democ_1900 + latit, data = data_base)

#Show results:
stargazer(reg_2_1, reg_2_2, type = "html")
```

Column $(1)$ uses constraints faced by the executive in 1900 as the regressor, and shows a close association between early institutions and institutions today. For example, past institutions alone explain $24\%$ of the variation in the index of current institutions. The second column uses the democracy index as a proxy for early institutions instead of constraints on executive, and confirm the results (with even higher R-squared).

*Note: Both constraints on the executive and democracy indices assign low scores to countries that were colonies in 1900, and do not use the earliest post-independence information for Latin American countries and Neo-Europes. Authors also use constraints on the executive in the first year of independence (in case the country was not yet independent in 1900) controlling separately for time since independence as a third variant of the regression. The results are similar and indicate that early institutions persist.*

Alright, we saw that there is a positive correlation between current and early institutions. But can we proceed with it as an instrument?


Quiz: Can we use early institutions as an instrument?

[1]: Yes, both relevance and endogeneity conditions are fulfilled.
[2]: No, relevance condition can be violated.
[3]: No, exogeneity condition can be violated.
[4]: No, both relevance and exogeneity conditions can be violated.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Choice of instrument 1")
```

If we incorporate the violation of the exogeneity condition on the previous DAG, it will look like this:
```{r "8_4"}
dag_3 <- dagitty('dag {   C -> I <-> Y <- v -> C <- X
                         v -> I <- X -> Y
  I [pos="0,0"]
  v [pos="0,1"]
  Y [o, pos="1,0"]
  X [pos="0,-0.5"]
  C [pos="-0.5,0.1"]
   }') %>% 
  dag_label(
    labels = c(
      "I" = "Quality of Institutions", 
      "Y" = "Level of Income", 
      "v" = "Error",
      "X" = "Geography",
      "C" = "Early institutions"))

ggdag(dag_3, use_labels = "label")
```

At this step, it is now obvious that we should search further.

### Early institutions and settlements

We have already mentioned well-functioning and extractive institutions earlier. If we dig deeper here in order to better understand how they originated, we will find that each colonization policy could be attributed to a specific type with defined features, which later led to the creation of different sets of institutions. Two main types were “extractive states” and “settler colonies”.
- The main purpose of the extractive state was to get as many resources of the colony as possible and to transfer them to the colonizer, preferably with the minimum amount of investment possible, a pure “short-term gain” for colonizers. Therefore, they did not develop or protect private property, nor did they provide checks and balances against government expropriation.
- As concerns settler colonies, people were trying to model the life there after their home country and to replicate European institutions, with great emphasis on private property, and checks against government power. Denoon (1983) confirms that settler colonies developed representative institutions which promoted what the settlers wanted and that what they wanted was freedom and the ability to get rich by engaging in trade. Main examples of former settler colonies include Australia, New Zealand, Canada, and the United States of America.

Overall, a key determinant of the form colonialism took was the presence or absence of European settlers. Therefore, we can incorporate a measure of European settlements in the colony proxied by a fraction of the population of European descent in 1900.

Then we can further enlarge the DAG by adding this variable:
```{r "8_5"}
dag_4 <- dagitty('dag {   S -> C -> I <-> Y <- v -> C <- X -> S
                         v -> I <- X -> Y
  I [pos="0,0"]
  v [pos="0,1"]
  Y [o, pos="1,0"]
  X [pos="-0.4,-0.5"]
  C [pos="-0.75,0.1"]
  S [pos="-1.5,0"]
   }') %>% 
  dag_label(
    labels = c(
      "I" = "Quality of Institutions", 
      "Y" = "Level of Income", 
      "v" = "Error",
      "X" = "Geography",
      "C" = "Early institutions",
      "S" = "European settlements"))

ggdag(dag_4, use_labels = "label")
```

Now let us build a necessary regression in order to check whether we could confirm the correlation of early institutions and settlements.

The regression equation should have the following structure: $$Early\ institutions_{i}=\beta_{0}+\beta_{1} \cdot European\ settlements_{i} + X'_{i} \cdot \gamma+\theta_{i}$$, where $X_i$ is a vector of other added covariates (geography).

Just press *check*.
```{r "8_6",results='asis'}
reg_2_4 = lm(cons_1900 ~ eur_1900 + latit, data = data_base)
reg_2_5 = lm(democ_1900 ~ eur_1900 + latit, data = data_base)

stargazer(reg_2_4, reg_2_5, type = "html")
```

The results of the regressions provide evidence in support of the hypothesis that early institutions were shaped, at least in part, by settlements. These regressions show that settlement patterns explain around $50$ percent of the variation in early institutions. Nevertheless, this variable also cannot be taken as an instrument due to the same reason as with early institutions - although it is relevant, it is likely not exogenous. For example, if Europeans were more likely to migrate to places with better resources and soil quality, and if resources and soil quality still had an effect on income, there would be a correlation between $European\ settlements$ (`eur_1900`) and $\Theta$ (Random error). 

If we incorporate the violation of the exogeneity condition on the previous DAG, it will look like this:

Just press *check*.
```{r "8_7"}
dag_5 <- dagitty('dag {   S -> C -> I <-> Y <- v -> C <- X -> S <- v
                         v -> I <- X -> Y
  I [pos="0,0"]
  v [pos="-0.4,0.8"]
  Y [o, pos="1,0"]
  X [pos="-0.4,-0.5"]
  C [pos="-0.75,0.1"]
  S [pos="-1.5,0"]
   }') %>% 
  dag_label(
    labels = c(
      "I" = "Quality of Institutions", 
      "Y" = "Level of Income", 
      "v" = "Error",
      "X" = "Geography",
      "C" = "Early institutions",
      "S" = "European settlements"))

ggdag(dag_5, use_labels = "label")
```

And again, we should perform one more step to reveal the next underlying factor. We have already reached the measure of European settlements, but we could look at what defined these settlements.

### Determinant of settlements


Quiz: Make a guess, what likely had the most impact on decisions where to make settlements?

[1]: The average temperature in the country
[2]: Soil quality
[3]: Distance to the potential colony
[4]: Colonists' mortality rate

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Choice of instrument 2")
```

Authors of the paper claim that the colonization strategy was influenced by the feasibility of settlements, i.e. extractive colonies were organized in places with less favorable disease environment to European settlement. At the same time, European colonialists brought better institutions to locations where they could safely settle.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Mortality data")
```

Although Europeans did not know how to control the diseases that caused these high mortality rates, there were very well informed about mortality rates faced by settlers at that time.

Let us reformulate the logical chain that we have come to so far. We assume that settler mortality rates were a major determinant of settlements; that settlements were a major determinant of early institutions (in practice, institutions in 1900); that there is a strong correlation between early institutions and institutions today; and finally that current institutions have a first-order effect on current performance.

It gives us the following DAG structure:
```{r "8_8"}
dag_6 <- dagitty('dag {   X -> M -> S -> C -> I <-> Y <- v -> C <- X -> S <- v
                         v -> I <- X -> Y
  I [pos="0,0"]
  v [pos="-0.4,0.8"]
  Y [o, pos="1,0"]
  X [pos="-0.4,-0.5"]
  C [pos="-0.75,0.1"]
  S [pos="-1.5,0"]
  M [pos="-2.5, 0.1"]
   }') %>% 
  dag_label(
    labels = c(
      "I" = "Quality of Institutions", 
      "Y" = "Level of Income", 
      "v" = "Error",
      "X" = "Geography",
      "C" = "Early institutions",
      "S" = "European settlements",
      "M" = "Mortality rate"))

ggdag(dag_6, use_labels = "label")
```

Let us then check the hypothesis that settler mortality rates defined European settlements with our standard methodology. But first, a short quiz.


Quiz: If we had to choose, would we prefer to use the original mortality rate or its logarithm?

[1]: Original form of mortality rate
[2]: Logarithm of mortality rate

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Choice of instrument 3")
```

The regression equation should have the following structure: $$European\ Settlements_{i}=\beta_{0}+\beta_{1} \cdot log\ (Mortality\ rate)_{i} + X'_{i} \cdot \gamma+u_{i}$$, where $X_i$ is a vector of other added covariates (geography).

Now let's continue with the code:
```{r "8_9",results='asis'}
#Regress measure of european settlements (variable `eur_1900`) on logarithm of mortality rate and include control variable for latitude. Then show summary with `stargazer()` command:
reg_2_6 = lm(___ ~ ___ + latit, data = data_base)
stargazer(___, type = "html")

```

We can see that the results are quite optimistic with relatively high R-squared and all variables being statistically significant. Moreover, we have a negative coefficient estimate for logarithm of mortality, which means that with $1$ log point increase of mortality, the fraction of European population is decreasing by $7.1$, which stands for percentage points in our case. 

Since we are choosing the instrument for the quality of institutions, we should check both the relevance and exogeneity conditions to be fulfilled.

**Relevance**

Although we have already seen necessary correlations in each stage of our chain of variables, it would also be useful to check the relation between mortality and the quality of institutions.

For this we can plot these 2 variables:

```{r "8_10"}
theme_set(theme_classic())
#Replace two ___ in aes() commands to plot log mortality rate on x-axis and protection against expropriation risk on y-axis. After that also recall the command to add the regression line to the plot and replace ___ in the correct line with it.
exprop_mort = data_base %>%
    ggplot(aes(x = ___, y = exprop)) +
    geom_point() + 
    labs(x="Log of settler mortality rate", y = "Average protection against expropriation risk 1995")+ 
    ___(method='lm', se = FALSE)

#Plot it:
exprop_mort

```

Now we run the regression to see more details:

```{r "8_11",results='asis'}
#Regress protection against expropriation risk on log settler mortaility:
reg_2_7 = lm(___ ~ ___ + latit, data = data_base)

#Show results
stargazer(reg_2_7, type = "html")

```


Both on the chart and in the regression results we can see an obvious pattern, which shows that ex-colonies where Europeans faced higher mortality rates have substantially worse institutions today. This notion confirms our assumption, so mortality fulfills the relevance condition.

**Exogeneity**

As concerns its exogeneity, this is a more sophisticated question. Our idea is to show that the mortality rates of European settlers more than 100 years ago have no effect on GDP per capita today, other than their effect through institutional development and conditional on the control variables included in the regression. The main problem is that we cannot say right away whether mortality rates could be correlated with other factors having a direct effect on the economic performance, and a more precise investigation is necessary.

As we have already mentioned, it is quite logical to assume that a major determinant of settler mortality was the disease environment. At the same time, these diseases could have a lot of long-lasting negative effects that are still affecting current economic performance through other channels. It could be the decrease of life expectancy, inability to properly develop the healthcare and education systems, and so on.

Authors state that deaths were mostly caused by malaria and yellow fever, but they also argue that these diseases had only limited effect on indigenous adults who had developed various types of immunities. They provide an example of Bengal and Madras (taken from Curtin (1968, Table 2)), where mortality rates of local troops were comparable to that of local British troops (11 and 13 compared to 15 in 1000), whereas when British troops came to these countries, their mortality rates jumped to numbers between 70 and 170 in 1000, what could be explained by lack of immunity, since these two diseases accounted for 80 percent of European deaths (Curtin, 1989 p. 30).

Furthermore, Acemoglu et al. (2001) show that these areas in the tropical zone were richer and more densely settled in 1500 than the temperate areas later settled by the Europeans. This also supports the notion that the disease environment did not create an absolute disadvantage for these countries, therefore the fact that many African and Asian countries are poor today cannot be explained solely and directly by the unfavorable disease environment.

Taking into account all of the above, we can logically assume that diseases affected European settlement patterns and the type of institutions they set up, but had little effect on the health and economy of indigenous people. Therefore, settler mortality fulfills both the relevance and exogeneity conditions and could be a plausible instrument for the quality of institutions.

*Note: at this stage, we are only taking into account logical considerations to make a conclusion about the validity of the instrument. In the next exercise, we will use some formal tests to check necessary conditions.*

## Exercise 4.3: IV regression

In this exercise, we will continue to investigate the impact of the quality of institutions on the economic performance by applying the instrumental variables estimation and the 2- stage least squares method.

In the previous chapter, we have already shown that the settler mortality could act as a plausible instrument for the quality of institutions, and if we use mortality as a source of exogenous variation for institutions, our DAG will have the following look:
```{r "9_1"}
theme_set(theme_dag())
dag_7 <- dagitty('dag {   X -> M -> I <-> Y <- v -> I <- X -> Y
  I [pos="0,0"]
  v [pos="0.4,0.8"]
  Y [o, pos="1,0"]
  X [pos="0,-0.5"]
  M [pos="-1, 0"]
   }') %>% 
  dag_label(
    labels = c(
      "I" = "Quality of Institutions", 
      "Y" = "Level of Income", 
      "v" = "Error",
      "X" = "Geography",
      "M" = "Mortality rate"))

ggdag(dag_7, use_labels = "label")
```

In order to make use of the valid instrument, **two-stage least squares (2SLS)** method can be applied. In the info block below you will find some more information about it.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("2SLS")
```

Now that we have the background theoretical information, we can think of how the 2SLS works in our case. In such ”two-stage” regression we follow the causal chain by breaking down the chain into its links:

- In the first stage, we will regress the settler mortality on the risk of expropriation (analogous to what we did at the end of the previous exercise).
- In the second stage, we then use the fitted risk of expropriation (as predicted in the first stage by settler mortality) and regress the log GDP per capita on this fitted risk of expropriation. 

Intuitively, you can think of the first stage as ”extracting” the exogenous part of institutions that we know is driven by settler mortality. Since the settler mortality is assumed to have no direct effect on current GDP, we can then - in the second stage - use the variation in institutions driven by settler mortality to explain current levels of GDP. Provided the instrument is valid (relevance and exogeneity met), we then arrive at a causal interpretation of the regression.

### First stage in 2SLS

Now let us begin with the first stage of the two-stage procedure in the most simple setup, and run the first regression. Although we have already performed these regressions in the previous exercise while assessing the relevance of the instrument, it is also useful to do the first stage explicitly and to have the results in front of us once again. Just press *check*:

```{r "9_2",results='asis'}
data_base = readRDS("data_base.RDS")
reg_3_1_1s = lm(exprop ~ log_mort + latit, data = data_base)
stargazer(reg_3_1_1s, type = "html")
```

We have already shown that the instrument is highly significant since its p-value is smaller than $0.01$.

`log_mort`'s coefficient estimate is $-0.510$ when regressing the expropriation risk. This implies that an increase in the settler mortality rate by one percent is associated with a decline in the quality of institutions score, measured with `exprop`, by $0.005$ units. This direction of the relationship is also in line with the general logic, as was discussed earlier.

### Second stage in 2SLS

As concerns the second stage, we will first perform it explicitly to demonstrate the logic of the approach.

```{r "9_3"}
#We use `fitted.values()` function to get the predicted by the model values of `exprop`
exprop.hat = fitted.values(reg_3_1_1s)
```

Now let us estimate the second stage regression and compare the result with the simple OLS.

Before we do so, try to guess whether the 2SLS estimated coefficient would be higher or lower than that in the OLS.


Quiz: Looking at the coefficient in front of the “exprop” variable, would the IV estimator be higher than the OLS estimator?

[1]: The OLS estimator should be biased upwards due to the reverse causality, omitted variables and institutions measured ex-post
[2]: The OLS estimator should be biased downwards due to the measurement error
[3]: The OLS estimator could be biased upwards or downwards, depending on which effect is stronger

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Educated guess")
```

Now let us check what we get with the regressions:

```{r "9_4",results='asis'}
#Now we use these predicted values instead of the original `exprop` data to perform the second stage regression:
reg_3_1_2s = lm(log_gdp_95 ~ exprop.hat + latit, data = data_base)

#Here we also replicate the original OLS
reg_3_1_OLS = lm(log_gdp_95 ~ exprop + latit, data = data_base)

#Showing the results next to each other:
stargazer(reg_3_1_OLS, reg_3_1_2s, type = "html")
```

The second stage shows the final regression, where we regress the logarithm of GDP on the expropriation risk (as predicted by the settler mortality). The corresponding 2SLS estimate of the impact of institutions on 1995 income per capita is $0.996$, which seems highly significant with a standard error of $0.222$.

However, this result is not entirely correct. Manual computation leads to invalid standard errors and test statistics, so we cannot make any conclusions about the statistical significance.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Why are standard errors wrong in this case?")
```

In order for the econometric package to calculate the standard errors of the coefficients correctly, you should always use the built-in 2SLS procedure. In our case, we will use function `ivreg()` from the package `ivreg`.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("How to use the ivreg() function?")
```

Now let us check what we get if we use the correct approach:

```{r "9_5",results='asis'}
#As usual, loading the library first:
library(ivreg)

#Now replace first ___ in the ivreg() command with the proper instrumental variable and second ___ with an exogenous variable that we should also always add after |. Refer to the info block about ivreg() command above, if necessary.
reg_3_1_2SLS = ivreg(log_gdp_95 ~ exprop + latit | ___ + ___, data = data_base)

#Showing the output to compare all 3 
stargazer(reg_3_1_OLS, reg_3_1_2s, reg_3_1_2SLS, type = "html")
```

We can see that `ivreg()` function yields exactly the same coefficient estimates, but different standard errors, which are now correct.

Before analyzing the bias and continuing with the 2 other statements, we will graphically show the change in coefficients and associated standard errors using the `modelplot()` function from `modelsummary` which plots the coefficient estimates along with their 95% confidence intervals.
Just press *check*:
```{r "9_6"}
library(modelsummary)
model_plot = modelplot(list("2SLS" = reg_3_1_2SLS, "OLS"= reg_3_1_OLS))
model_plot
```

Analyzing the table and the chart allows us to make 3 statements:

- An increase in the average risk of expropriation score by 1 unit now increases the GDP per capita by $(e^{0.996}-1) \cdot 100\%=170.7\%$.

- Similar to the OLS result, we find a strong positive relationship, but now the estimated coefficient is even larger than the OLS estimate. This suggests that the measurement error in the institutions variables that creates the attenuation bias is likely to be more important than reverse causality and omitted variables biases. “Measurement error” here is broadly construed. As discussed earlier, in reality the set of institutions that matter for economic performance is very complex, and any single measure is bound to capture only part of the “true institutions”, creating a typical measurement error problem.

- The latitude variable in 2SLS has the “wrong” sign and is insignificant. Based on this, the authors of the article suggest that many previous studies may have found latitude to be a significant determinant of economic performance because it is correlated with institutions (or with the exogenous component of institutions caused by early colonial experience). Nevertheless, we will continue to include latitude in our regressions to check if this assumption holds under other specifications.

### Tests

We would also like to have some formal confirmation that the instruments are indeed valid, that we actually face endogeneity problems in the OLS model, which will make us prefer 2SLS over OLS.

Three main tests commonly used are the Weak instruments test, Hausman test and Sargan test.

**Weak instruments test**

Weak Instruments are instrumental variables that are only slightly correlated with the relevant endogenous explanatory variable or variables.

If the instruments are weak, then:
- the accuracy of 2SLS estimates is very low;
- the results of significance tests may be incorrect, since the distribution of the coefficient estimate is not normal even asymptotically.

In practice, we can simply check the calculated value of the F statistic in the first-stage regression, and if it is higher than $10$, then the instruments are considered relevant.

The proof of this fact is technically quite complicated, and details can be found in the Stock, Yogo (2005).

In our case the F-statistic was equal to $12.8$, therefore the instruments should be considered relevant.

**Hausman test**

One very helpful method to check endogeneity and to justify the use of 2SLS is to perform the Hausman test. It helps to decide whether we need to use 2SLS in our model, or we can limit ourselves to the usual OLS.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Hausman test")
```

We can easily see the result of the Hausman test in the "Diagnostic test" section if we call `summary()` command with an option `diagnostics = TRUE`.

```{r "9_7"}
summary(reg_3_1_2SLS, diagnostics = TRUE)
```


Quiz: Based on the results of the Hausman test, what conclusion can we make in both cases?

[1]: There is no clear evidence that OLS performs worse than 2SLS
[2]: Although we might face endogeneity, we cannot say that 2SLS should be the preferred method, and further investigation is needed
[3]: We face clear endogeneity problems, and 2SLS is the viable method that should be used here

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Hausman test")
```


**Sargan test**

Sargan test for overidentification checks the exogeneity of instruments. The null hypothesis of the test is that all instruments are exogenous, an alternative hypothesis is that at least one of the instruments is endogenous.

The test is available only if the number of instruments exceeds the number of endogenous regressors.

In the constructed model, it is impossible to carry out the Sargan test for overidentification, since in this case, the number of instruments does not exceed the number of endogenous regressors (one endogenous regressor and one instrument). This means that the validity of the instrumental variables used can only be confirmed logically. Therefore, a meaningful understanding of the model is especially important if you are using 2SLS.

We will not go deep into the Sargan test here since we will get back to overidentification tests in the next exercise.

### Robustness checks with changing samples and concluding remarks
Finally, it would make sense to check if our regression results are driven by some specific regions in the sample. In order to do this, we could run several regressions and exclude potentially suspicious (in terms of impact on the result) areas one by one. Additionally, we run a regression with region dummy variables to see if there are any potential discrepancies.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Other additional controls and robustness checks")
```

Just press *check* to run the regressions and display the results:
```{r "9_8",results='asis'}
reg_3_2_2SLS = ivreg(log_gdp_95 ~ exprop + latit | log_mort + latit, data = subset(data_base, neo_europe == "0"))
reg_3_3_2SLS = ivreg(log_gdp_95 ~ exprop + latit | log_mort + latit, data = subset(data_base, africa == "0"))
reg_3_4_2SLS = ivreg(log_gdp_95 ~ exprop + asia + africa + other + latit | log_mort + asia + africa + other + latit, data = data_base)

stargazer(reg_3_1_2SLS, reg_3_2_2SLS, reg_3_3_2SLS, reg_3_4_2SLS, type="html")
```

Column $(1)$ corresponds to the original 2SLS regression, which we have already discussed earlier.
Column $(2)$ documents that our results are not driven by the Neo-Europes (authors include United States, Canada, Australia, and New Zealand in this group). When we exclude these countries, the estimates remain highly significant, and even increase a little. For example, the coefficient for institutions is now $1.21$ (s.e. = $0.35$).
Column $(3)$ shows that our results are also robust to dropping all the African countries from the sample. The estimates without Africa are somewhat smaller, but also more precise. For example, the coefficient for institutions is $0.58$ (s.e. = $0.1$). 


Quiz: Why could the coefficient in front of `exprop` go down in a sample without Africa?

[1]: Due to the considerably smaller number of countries included in the regression
[2]: Due to the lower variation in GDP and expropriation risk in all other countries
[3]: Due to the higher variation in GDP and expropriation risk in all other countries
[4]: Cannot be estimated based on the available information

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Africa")
```

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("What if the sample is only limited to Africa?")
```

Column $(4)$ comprises the regression with continent dummies for Africa, Asia, and other, with America being the omitted group. The addition of these dummies does not change the estimated effect of institutions, and the dummies are jointly insignificant at the 5-percent level, though the dummy for Asia is significantly different from that of America. The fact that the African dummy is insignificant suggests that the reason why African countries are poorer is not due to cultural or geographic factors, but mostly accounted for by the existence of worse institutions in Africa.

**Overall, these results show a large effect of institutions on economic performance, which was the main aim of the authors of the paper.**


## Exercise 4.4: Effects of diseases, overidentification tests & criticism

### Effects of diseases and health characteristics

We have already discussed the potential effects of diseases in the section about the exogeneity of the instrument, but here we are going to delve deeper and analyze various regression specifications to investigate whether the instrument could be capturing the general effect of disease environment and other health characteristics of the countries on development.

**Malaria, life expectancy and infant mortality** 

Many authors have argued for the importance of malaria and other diseases in explaining African poverty (Bloom and Sachs, 1998; Gallup and Sachs, 1998; Gallup et al., 1998). Since malaria was one of the main causes of settler mortality, our estimate may be capturing the direct effect of malaria on economic performance.

However, the authors of the paper are skeptical of this argument since malaria prevalence is highly endogenous; it is the poorer countries with worse institutions that have been unable to eradicate malaria. While Sachs and coauthors (1998) argue that malaria reduces output through poor health, high mortality, and absenteeism, most people who live in high malaria areas have developed some immunity to the disease. For a person without immunity, malaria is often fatal, so Europeans in Africa, India, or the Caribbean faced very high death rates. In contrast, death rates for the adult local population were much lower (Curtin (1964)).

Malaria should therefore have very high social costs, but little direct effect on economic performance. In contrast, for Europeans, or anyone else who has not been exposed to malaria as a young child, malaria is usually fatal, making malaria prevalence a key determinant of European settlements and institutional development.

As concerns health characteristics, authors use life expectancy and infant mortality.

Let us check what impact would controlling for malaria, life expectancy and infant mortality have: 

```{r "10_1",results='asis'}
#Loading data
data_base = readRDS("data_base.RDS")

#Running regressions
reg_base_2SLS = ivreg(log_gdp_95 ~ exprop + latit | log_mort + latit, data = data_base)
reg_mal_2SLS = ivreg(log_gdp_95 ~ exprop + mal_94 + latit | log_mort + mal_94 + latit, data = data_base)
reg_lifeexp_2SLS = ivreg(log_gdp_95 ~ exprop + life_exp_95 + latit | log_mort + life_exp_95 + latit, data = data_base)
reg_infmort_2SLS = ivreg(log_gdp_95 ~ exprop + inf_mort_95 + latit | log_mort + inf_mort_95 + latit, data = data_base)

#Showing results
stargazer(reg_base_2SLS, reg_mal_2SLS, reg_lifeexp_2SLS, reg_infmort_2SLS, type = "html")
```

The estimates show a significant effect of institutions on income, which is similar to, but smaller than baseline estimates.

Since malaria prevalence in 1994 is highly endogenous, controlling for it directly should underestimate the effect of institutions on performance (see Appendix A in the original paper). The coefficient on protection against expropriation is now estimated to be somewhat smaller, 0.72 instead of 0.996 in the original 2SLS regression.

Authors claim that infant mortality is marginally significant. In our estimates, it is not significant at the $10\%$ level but is close to it, which might be due to marginal reporting differences. Anyway, this is not sufficient to make any conclusions about the impact of infant mortality.

Since health is highly endogenous, the coefficient on these variables is biased up, while the coefficient of institutions is biased down. These estimates are therefore consistent with institutions being the major determinant of income per capita differences, with little or no effect from health variables, as stated in Acemoglu et al. (2001).

Overall, the effect of institutions remains statistically significant, while malaria and other control variables are insignificant.

*Authors also adopt an alternative approach by treating both health and institutions are treated as endogenous and are instrumented for using latitude, average temperature, and amount of territory within 100 km of the coast in addition to the original instrument, settler mortality. The results are very similar.*

**Yellow fever**

Although yellow fever’s epidemiology is quite different from malaria, it was also much more fatal to Europeans than to non-Europeans who grew up in areas where yellow fever commonly occurred. The yellow fever leaves its surviving victims with a lifelong immunity, which also explains its epidemic pattern, relying on a concentrated non-immune population.

As a final strategy to see whether settler mortality could be proxying for the current disease environment, authors estimated models using a yellow fever instrument (dummy variable indicating whether the area was ever affected by yellow fever).

Here we also show the first-stage regression to make the interpretation more clear:
```{r "10_2",results='asis'}
reg_yellfev_1s = lm(exprop ~ yell_fev, data = data_base)
reg_yellfev_2SLS = ivreg(log_gdp_95 ~ exprop | yell_fev, data = data_base)
stargazer(reg_yellfev_1s, reg_yellfev_2SLS, reg_base_2SLS,  type = "html")
```

The authors assess the yellow fever results as encouraging, as the estimate in the base sample is 0.91 comparable to the baseline estimate of $0.996$. In the first stage regression we see a negative $-1.083$ estimate for the coefficient in front of yellow fever, which implies that countries affected by yellow fever have lower protection against expropriation risk.

Authors argue that it is an attractive alternative strategy since yellow fever is mostly eradicated today, which means that the dummy should not be correlated with the current disease environment. 

However, the disadvantage of this approach is that there is less variation in this instrument than in the settler mortality variable. Controlling for latitude makes the IV estimates insignificant, which is likely due to this lack of variation as well as it being strongly correlated with latitude. Moreover, F-statistic in the first stage regression is $7.169$, which makes it a weak instrument.

Overall, it is only partially suitable even for validation purposes.

### Overidentification  tests

Overidentification tests could be also used to investigate the validity of the approach. Here we would like to check whether `European settlements` or `Early institutions` (which we have faced in the "Choice of plausible instrument" section) have a direct effect on income per capita by using additional instruments.

First of all, we replicate an easy-to-interpret version of the overidentification test. It adds the log of mortality as an exogenous regressor, and if mortality rates faced by settlers had a direct effect on income per capita, we would expect this variable to come in negative and significant.

Just press *check*:
```{r "10_3",results='asis'}
#Run IV regressions with logarithm of mortality as exogenous variables in addition to expropriation risk treated as endogenous:

reg_eur_mort_2SLS = ivreg(log_gdp_95 ~ exprop + log_mort + latit | eur_1900 + log_mort + latit, data = data_base)
reg_cons_mort_2SLS = ivreg(log_gdp_95 ~ exprop + log_mort + latit | cons_1900 + log_mort + latit, data = data_base)
reg_democ_mort_2SLS = ivreg(log_gdp_95 ~ exprop + log_mort + latit | democ_1900 + log_mort + latit, data = data_base)

#Check results:
stargazer(reg_eur_mort_2SLS, reg_cons_mort_2SLS, reg_democ_mort_2SLS, type = "html")
```

As you can see, `log_mort` is negative, but at the same time small and statistically insignificant in all cases. This gives a reason to assert that the impact of mortality rates faced by settlers likely works through their effect on institutions, but not through other channels.

Now let's perform a **Sargan test**, which we were not able to in one of the previous exercises since we had not enough instruments (remember that it can only be applied if we have at least one more excluded instrument than endogenous variables). We are not going to cover mathematical explanation or detailed testing procedure, but more information could be found in e.g. Wooldridge (2018), ch. 15.

We need to remember that the null hypothesis H0 is that all instruments are exogenous, and if the p-value is low (i.e. the H0 is rejected), it suggests that at least one instrument is endogenous.

However, the most tricky part is that the Sargan test may well fail to detect if all instruments are endogenous and correlated with each other. Keeping this in mind, let us check the results.

*Since showing summary for each regression takes a lot of space, we only show the results of one Sargan test. Other results can also be shown if you uncomment respective rows.*
```{r "10_4"}
#Run regressions with additional instruments
reg_cons_2SLS = ivreg(log_gdp_95 ~ exprop + latit | log_mort + cons_1900 + latit, data = data_base)
#reg_democ_2SLS = ivreg(log_gdp_95 ~ exprop + latit | log_mort + democ_1900 + latit, data = data_base)
#reg_eur_2SLS = ivreg(log_gdp_95 ~ exprop + latit | log_mort + eur_1900 + latit, data = data_base)

#Check `diagnostics` section in the summaries:
summary(reg_cons_2SLS, diagnostics = TRUE)
#summary(reg_democ_2SLS, diagnostics = TRUE)
#summary(reg_eur_2SLS, diagnostics = TRUE)
```

The results of the Sargan tests are quite ambiguous. P-values are rather high, so we do not reject the null hypothesis and the instruments could be considered exogenous. However, we know that they are correlated and we also expected variables reflecting "European settlements" and "Early institutions" to be endogenous. This might suggest that:
- all instruments are actually exogenous, but this would mean that our initial expectations about other instruments are not realistic enough 
- all instruments are endogenous and the Sargan test fails to capture it
- the data quality is simply not good enough to make meaningful conclusions

We will also extend the author’s analysis and perform an additional Sargan test with yellow fever as an additional instrument.

```{r "10_5"}
#Run regressions with yellow fever as an additional instrument
reg_yf_2SLS = ivreg(log_gdp_95 ~ exprop + latit | log_mort + yell_fev + latit, data = data_base)
#Show Sargan test result
summary(reg_yf_2SLS, diagnostics = TRUE)
```

The results are very similar to what we have seen in the previous tests, and we could not get closer to one and the only conclusion we can be sure about. In any case, we are facing potential problems that require proper investigation, and we will not dive deeper into it now, as this is not the main goal of this Rtutor problem set.



### Criticism

Along with the recognition of the Acemoglu et al. (2001) work, there are a number of papers and articles that criticize it.

The most prominent criticisms come from Albouy (2004, 2006, 2012) who claims that the settler mortality rates used by Acemoglu et al. suffer from measurement error.

Albouy criticizes the data due to the fact that only 28 of the 64 countries in their original sample have actual data; whereas the other 36 countries are assigned rates based on conjectures made by the authors on the similarity of health conditions such as the disease environment. Therefore, Albouy drops these observations and finds that the effect of settler mortality on expropriation risk is much smaller, making it a weak instrument to study the causal effect of institutions on growth. Moreover, Albouy proposes to introduce a coding for whether or not the mortality data are drawn from a military campaign.

In reply, Acemoglu et al. (2005, 2006, 2012) claim that their estimates of the data are supported by historical records and are therefore reliable, whereas Albouy‘s data suffer from selection biases based on irrational conjectures. Acemoglu et al. also show that even minor corrections to the way in which he codes “campaign” dummy restores the robustness of original results, and limiting the effect of very high mortality rates largely restores the robustness of the results even without correcting the inconsistencies in Albouy’s coding.

Glaeser et al. (2004) argue that instrumenting institutions by European settler mortality violates the exogeneity assumption on the basis that the colonizers brought with them skills and human capital that persisted over time. They claim that it is the persistence of these skills that is causing the variation in output per worker today, and not the institutions. They run a simple OLS regression of GDP per capita on executive constraints, as well as controlling for additional variables, to show that human capital has been the main determinant of growth since the colonial period, and not institutions.

**Overall, there is no single clear answer on how accurate are the conclusions made by Acemoglu et. al (2001) in their paper, but neither there is in any other research. Nevertheless, this is not surprising since the topic itself is rather complicated and revealing the actual picture is almost impossible.**

**Authors still treat institutions as a “black box” and highlight: “It is useful to point out that our findings do not imply that institutions today are predetermined by colonial policies and cannot be changed. We emphasize colonial experience as one of the many factors affecting institutions.”**


## Exercise 5.1: 20 years later - introduction

As discussed in the first exercise, we would also like to check if the original conclusions hold when we use modern data. In this exercise, we will analyze modern data for expropriation risk and log GDP per capita that is included in our dataset and compare the results with the original conclusions of the paper, written 20 years ago.

Before we start, we should load the dataset again, as usual:

```{r "11_1"}
data = readRDS("data.RDS")
head(data)
```


### Variables details

We have already included modern data in the dataset, and here we are interested in variables `exprop_21` and `log_gdp_19`:
- `exprop_21` variable is taken from Credendo, European credit insurance group, which is active in all segments of trade credit and political risk insurance and provides a range of products that cover risks worldwide. Credendo holds a regularly updated database estimating country risk with a number of parameters, including expropriation risk. In this dataset values provided by Credendo are inverted for convenience of comparison with the original dataset used in the paper, and are in a range from 1 to 7, where a higher score means less risk. Data can be downloaded <a href="https://credendo.com/en/country-risk" target="_blank">here</a>.


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("How risk of expropriation was estimated by Credendo?")
```

- `log_gdp_19` is a logarithm of Gross Domestic Product per capita in year 2019, Purchasing Power Parity Basis. Data is taken from World Bank, as it was also done in the original paper. World Bank PPP GDP can be found <a href="https://data.worldbank.org/indicator/NY.GDP.MKTP.PP.CD" target="_blank">here</a>.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Why are we using 2019 GDP and 2021 expropriation risk?")
```

### Descriptive analysis of modern data

**GDP per capita**

Since now we will analyze data for 2019, we can also plot it to see how the distribution changed in 24 years. Note that we are plotting 4 charts, 2 of which are necessary to compare log GDP distribution and 2 for unadjusted GDP data in 1995 and 2019. We also fix x- and y- axes limits to be able to directly compare the charts and see the differences.

Now just press *check*.

```{r "11_2",fig.height=7, fig.width=8}
library(ggplot2)
library(gridExtra)
theme_set(theme_classic())

histogram_GDP_95 = ggplot(data, aes(x = log_gdp_95)) + geom_histogram(fill="orange", colour="black", binwidth = 0.5) + geom_vline(xintercept=mean(data$log_gdp_95, na.rm=TRUE), size = 1, colour = "red", linetype = "dashed") + labs(title="Log GDP Distribution in 1995", x="Log PPP GDP 1995", y = "Count") + theme(plot.title = element_text(hjust = 0.5)) + xlim(5, 12) + ylim(0,45)

histogram_GDP_95_exp = ggplot(data, aes(x = exp(log_gdp_95))) + geom_histogram(fill="lightblue", colour="black", binwidth = 2000) + geom_vline(xintercept=mean(exp(data$log_gdp_95), na.rm=TRUE), size = 1, colour = "red", linetype = "dashed") + labs(title="GDP Distribution in 1995", x="PPP GDP 1995", y = "Count") + theme(plot.title = element_text(hjust = 0.5)) + xlim(0, 80000) + ylim(0,45)

histogram_GDP_19 = ggplot(data, aes(x = log_gdp_19)) + geom_histogram(fill="orange", colour="black", binwidth=0.5) + geom_vline(xintercept=mean(data$log_gdp_19, na.rm=TRUE), size = 1, colour = "red", linetype = "dashed") + labs(title="Log GDP Distribution in 2019", x="Log PPP GDP 2019", y = "Count") + theme(plot.title = element_text(hjust = 0.5)) + xlim(5, 12) + ylim(0,45)

histogram_GDP_19_exp = ggplot(data, aes(x = exp(log_gdp_19))) + geom_histogram(fill="lightblue", colour="black", binwidth=2000) + geom_vline(xintercept=mean(exp(data$log_gdp_19), na.rm=TRUE), size = 1, colour = "red", linetype = "dashed") + labs(title="GDP Distribution in 2019", x="PPP GDP 2019", y = "Count") + theme(plot.title = element_text(hjust = 0.5)) + xlim(0, 80000) + ylim(0,45)

#Plot them in a 2x2 matrix to compare the changes directly:
grid.arrange(histogram_GDP_95, histogram_GDP_95_exp, histogram_GDP_19, histogram_GDP_19_exp, ncol=2)
```

As you can see on the charts, the modern 2019 data is significantly skewed right compared to 1995, which is obviously due to organic development of the world and countries’ economies in the last 20 years and due to inflation, since current prices are used.

More interestingly, the nature of the distribution has changed. We still see something similar to the lognormal distribution in 2019, however, there are much fewer “very poor” countries and the right tail is much longer. This reflects the decrease in inequality and the more even distribution of income across countries.

Let us also check how things have changes for each continent. For convenience we will only have a quick look at the medians. 

```{r "11_3"}
#As usual, first we need the respective package to make use of its commands.
library(dplyr)

#New we want use group_by() and summarize() commands to group the data by continents:

data[!is.na(data$log_gdp_95)&!is.na(data$log_gdp_19),] %>% 
  group_by(continent) %>% 
  summarise(Median_GDP_per_capita_95 = round(median(exp(log_gdp_95)), digits = 0), Median_GDP_per_capita_19 = round(median(exp(log_gdp_19)), digits = 0))

```

As you can see, the variation between countries on different continents in both 1995 and 2019 is quite significant. In 2019 we see a 2- to 5-fold increase in median GDP per capita with continents holding similar relative positions as in 1995. We also notice that Asia has experienced the highest growth over these years (as % change), which also matches our expectations since Asian countries have seen a surge in foreign investments and were, in general, developing faster than any other region over the last 20 years.

**Protection against expropriation risk**

Let us now compare the country’s ratings for protection against expropriation risk. We adopt a similar approach and compare the data with the help of histograms and same axes scales.

```{r "11_4"}
#Plot original expropriation risk (variable `exprop`) on the histogram
histogram_exprop_95 = ggplot(data, aes(x = exprop)) + geom_histogram(fill="darkmagenta", colour="black", binwidth=0.5) + geom_vline(xintercept=mean(data$exprop, na.rm=TRUE), size = 1, colour = "red", linetype = "dashed") + labs(title="Protection against expropriation risk (old)",x="Protection against expropriation risk", y = "Count") + theme(plot.title = element_text(hjust = 0.5)) + xlim(0,12) + ylim(0,40)

#Plot new measure of expropriation risk (variable `exprop_21`) on the histogram
histogram_exprop_21 = ggplot(data, aes(x = exprop_21)) + geom_histogram(fill="darkmagenta", colour="black", binwidth=0.5) + geom_vline(xintercept=mean(data$exprop_21, na.rm=TRUE), size = 1, colour = "red", linetype = "dashed") + labs(title="Protection against expropriation risk (new)", x="Protection against expropriation risk", y = "Count") + theme(plot.title = element_text(hjust = 0.5)) + xlim(0,12) + ylim(0,40)

#Now plot both histograms side by side
grid.arrange(histogram_exprop_95, histogram_exprop_21, ncol=1)

library(skimr)
skim(data)
```

First thing that we see there is that the author’s data for 1995 is much more granular, therefore there are no empty breaks between columns and the distribution is more smooth.

At the same time, we can clearly see that both sources assess the expropriation risk similarly in terms of distribution patterns. As discussed in the first exercise, there are 2 peaks at around average and maximum scores, reflecting groups of developing and developed countries, with a lower number of countries receiving extremely low or above-average ratings. This gives us hope that the expropriation risk was similarly estimated by Credendo as it was done in the original paper.

Finally, we see that the scale is slightly different: it varies from 0 to 10 in the original variable and from 0 to 7 in the modern representation. This also results in different average values.


Quiz: Would it make sense to use these values for expropriation risk in the regressions and directly compare the estimators?

[1]: Yes, since the distribution patterns are similar and we have the same logical idea behind both the original and modern data.
[2]: No, since we have different scales with different means and standard deviations.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Its a trap 5")
```

Alright, let us then standardize the variables:

```{r "11_5"}
#Standardization procedure:
data$exprop_95_st = (data$exprop - mean(na.omit(data$exprop)))/(sqrt(var(na.omit(data$exprop))))
data$exprop_21_st = (data$exprop_21 - mean(na.omit(data$exprop_21)))/(sqrt(var(na.omit(data$exprop_21))))

#Check that everything is done properly:
data %>%
  summarise(Mean_95 = round(mean(na.omit(exprop_95_st)), digits = 0),
            SD_95 = round(sqrt(var((na.omit(exprop_95_st)))), digits = 0),
            Mean_21 = round(mean(na.omit(exprop_21_st)), digits = 0),
            SD_21 = round(sqrt(var((na.omit(exprop_21_st)))), digits = 0))
```

With the standardized protection against expropriation risk, we can easily compare the regressions and interpret the differences as the coefficients would reflect the predicted change in GDP when the expropriation risk rating changes by 1, i.e. by the standard deviation.

## Exercise 5.2: 20 years later - regressions

### OLS regression

First of all, we present a visual representation of the OLS regressions to check how close we are 20 years to the original paper in the most simple case.

In the code chunk below we both replicate the chart from the paper and add a new one with modern data.

```{r "12_1"}

#Load a base sample with standardised variables
data_base_st = readRDS("data_base_st.RDS")

#Construct plots showing standardised expropriation risk and gdp values across the base sample

plot_1 = data_base_st %>%
    ggplot(aes(x = exprop_95_st, y = log_gdp_95)) +
    geom_text(label=data_base_st$country) + 
    labs(x="Protection against expropriation risk 1995", y = "Log PPP GDP 1995") +
    geom_smooth(method='lm', se = FALSE) +
    ylim(6,12) +
    xlim(-2,2)


plot_2 = data_base_st %>%
    ggplot(aes(x = exprop_21_st, y = log_gdp_19)) +
    geom_text(label=data_base_st$country) + 
    labs(x="Protection against expropriation risk 2021", y = "Log PPP GDP 2019") +
    geom_smooth(method='lm', se = FALSE) +
    ylim(6,12) +
    xlim(-2,2)

  
#Show them next to each other
grid.arrange(plot_1, plot_2, ncol=2)

```

Comparing the charts, we see a very similar picture. The line on the right chart is shifted upwards, obviously reflecting a positive change of countries’ GDP per capita, which means that we should see a higher constant in the regression results.

More importantly, the slope is quite similar, which implies that the original conclusions about the correlation of these variables are likely to hold at this point (remember that we cannot say anything about the causal relationship so far).

Let us know compare the OLS regression results:

```{r "12_2",results='asis'}
#First we replicate the original OLS
OLS_old = lm(log_gdp_95 ~ exprop_95_st + latit, data = data_base_st)

#Now we build a new OLS using the modern data
OLS_new = lm(log_gdp_19 ~ exprop_21_st + latit, data = data_base_st)

#Showing the results next to each other:
library(stargazer)
stargazer(OLS_old, OLS_new, type = "html")
```

The OLS results are quite similar. As expected, we have a bigger constant and the coefficient of interest in front of “Protection against expropriation risk” has not changed much. Interestingly, latitude is not significant in the modern regression and R-squared is lower, which might be because of more precise and granular data used by the authors in the original paper.

Since we know that OLS does not capture the causal relationship adequately, we move to the IV regression straight away.

### IV regression

We will use the same instrument, logarithm of settler mortality. 

Recalling the conditions for it to be valid, it should be **relevant** and **exogenous**. 

The same arguments as before apply to prove the exogeneity since we have only taken GDP data 20 years later and other fundamental conditions have not changed.

As concerns relevance, we also apply the same approach as before to check the relation between modern expropriation risk data and mortality.

For this we can plot these 2 variables and run the OLS regression:

```{r "12_3"}
#Here we build a plot with log mortality rate on x-axis and protection against expropriation risk on y-axis. We also add the regression line to the plot.
exprop_mort_old = data_base_st %>%
    ggplot(aes(x = log_mort, y = exprop_95_st)) +
    geom_point() + 
    labs(x="Log of settler mortality rate", y = "Protection against expropriation risk 1995") + geom_smooth(method='lm', se = FALSE)

exprop_mort_new = data_base_st %>%
    ggplot(aes(x = log_mort, y = exprop_21_st)) +
    geom_point() + 
    labs(x="Log of settler mortality rate", y = "Protection against expropriation risk 2021") + geom_smooth(method='lm', se = FALSE)


#present both charts side by side
grid.arrange(exprop_mort_old, exprop_mort_new, ncol = 2)

```

Here we see that the regression should be very similar - the slope is almost the same although the data is more fragmented and categorical. Let’s check it:

```{r "12_4",results='asis'}
#Here we run respective regressions.
reg_exprop_mort_old = lm(exprop_95_st ~ log_mort + latit, data = data_base_st)
reg_exprop_mort_new = lm(exprop_21_st ~ log_mort + latit, data = data_base_st)

stargazer(reg_exprop_mort_old, reg_exprop_mort_new, type = "html")
```

Great news, using modern data has confirmed the relevance of the instrument and even showed higher R-squared and F-statistic. Moreover, the F-statistic is equal to 13.0, therefore the instrument passes the **weak instruments test** and could be actually considered relevant. This once again shows that ex-colonies where Europeans faced higher mortality rates have substantially worse institutions today.

Now it’s time to run IV regressions and check whether the original results actually hold 20 years later.

```{r "12_5",results='asis'}
library(ivreg)
reg_2SLS_old = ivreg(log_gdp_95 ~ exprop_95_st + latit | log_mort + latit, data = data_base_st)
reg_2SLS_new = ivreg(log_gdp_19 ~ exprop_21_st + latit | log_mort + latit, data = data_base_st)

stargazer(reg_2SLS_old, reg_2SLS_new, type = "html")
```

The results are rather surprising. Using standardized modern data gives an even higher estimate of the coefficient in front of the expropriation risk. Change in the standardized 1995 protection against expropriation risk (`exprop_95_st`) by 1 standard deviation (i.e. by 1 unit) would imply approx. 6-fold change in PPP GDP per capita whereas modern data suggests a 7-fold difference.

As in the original IV regression, the latitude variable in 2SLS has the “wrong” sign and is insignificant. This once again supports the authors’ idea that latitude could be correlated with institutions or with the exogenous component of institutions caused by the early colonial experience.

Moreover, R-squared is negative. However, we should not interpret it. This happens because of the IV methodology - when the endogenous variables are regressed on the exogenous variables and the predicted values are then used as covariates in the second stage, the error that is minimized in the second stage is not the same as the error used to calculate the residual sum of squares ($R^2 = 1 - \frac{Residual\ Sum\ of\ Squares}{Total\ Sum\ of\ Squares}$). Consequently, the residual sum of squares can be less than the total sum of squares, which implies that the R-squared has become meaningless.

We will not run different regression specifications playing around with the sample and control variables here. However, the regressions were additionally built out of the scope of this problem set, and they showed that the coefficients are significant (at $1\%$ level except for the regression with continent dummy variables, where it is significant at $10\%$ level) and hold around $2$, as it is in the base case IV regression.

```{r "12_6",results='asis'}

```


## Exercise 6: Conclusion

The aim of this interactive problem set was to find out whether the differences in GDP per capita between countries can be explained by historical differences in institutions and state policies, as better institutions, more secure property rights, and less distortionary policies lead to the more favorable economic environment and a greater level of income.

Although the notion that the causal relationship between quality of institutions and economic performance finds support from many economists and social scientists, there are many varying opinions on what determines institutions and government attitudes towards economic progress. In order to estimate the effect of institutions on economic performance, one needs to isolate exogenous sources of variation, which is not an obvious task due to the uncertainty mentioned above.

In this set of exercises, we have replicated the paper **The Colonial Origins of Comparative Development: An Empirical Investigation** by D. Acemoglu, S. Johnson, and Jamea A. Robinson (2001), where the authors argue that differences in colonial experience could be a valid source of exogenous differences in institutions.

We started with descriptive statistics and graphical analysis in order to introduce the data provided for 163 countries, 64 of which were colonies in the past. We have learned how to use R functionality to build nice histograms and interactive charts as well as group data to find interesting patterns. We have found a considerable correlation between the quality of institutions (expropriation risk) and economic performance (PPP GDP per capita), as well as examined other bivariate relations between variables.

Then we have looked at the OLS regression to confirm our ideas about these relations and understood that the model is subject to numerous endogeneity problems. With the help of the backward induction, we have followed the authors’ logic to get back in the past and find that settler mortality could be a source of exogenous variation in institutions. Using it as an instrument, we have built instrumental variables regressions to find a positive causal relationship between the quality of institutions and economic performance.

Although we have also replicated some of the robustness checks and endogeneity tests, the quality of the data, authors’ argumentation about the exogeneity of instruments, and assumptions used at each step could be questioned, as it was done by Albouy (2004, 2006, 2012) or Glaeser et al. (2004).

In the last exercises of the problem set, we have extended the original paper by including modern data and got very similar results. Our results confirm the original conclusions, however, the same questions to the validity of the approach also remain. Overall, it is hardly conceivable that an agreement on such a topic would be reached any time soon, partially due to the complexity of the world and numerous factors affecting institutions and economic performance of different countries, partially due to the extremely long time frame, and partially due to the imperfection of available data.

Eventually, despite the presence of various opinions and points of view on this question, this paper represents a worthwhile attempt to get closer to the point when the “true” relation and causality will be revealed.


Here you can display all awards that you collected in the course of this problem set. Just press *edit* and *check* afterwards.

```{r "13_1"}
awards()
```

## Exercise 7 (optional): Data preparation

This is an additional exercise.

Many problem sets already offer pre-processed datasets, but the stage of data preparation is a very important part of any research which often requires substantial time and effort, especially if the data is not readily available.

In this exercise, you can learn how to prepare the data for further analysis step by step. Our case is still simplified, however, it gives a glimpse of this process.

First, we are going to import the necessary data in the raw form. As we are starting to replicate the paper from scratch, we will have to deal with the data packed for STATA format, i.e. *.dta* files. Nevertheless, R allows to load external packages with many useful features for data analysis and programming, and we will certainly take advantage of that. The necessary package is called `foreign`, which gives us a valuable command `read.dta()` to import *.dta* files into R. 

In this task you should first use the command `library()` to access the package `foreign`. You simply have to put the name of the package in brackets of the command without any quotes. After that import a first piece of our data, which is named `Maketable1.dta`. Just use the command `read.dta()` with `Maketable1.dta` as an argument in quotes and assign it to the variable `table_1`.

```{r "14_1"}
# Load the package `foreign`. Type the code in the empty line below:

# Now assign the data maketable1.dta to the variable table_1. Enter your code below:

```

Now let's have a look at what the authors provide in this table. Use the command `head()` with loaded table name as an argument to reveal first few rows of the dataset.

```{r "14_2",optional=TRUE}
#Let's have a look at the data with the head() command. Enter your code here:

```

Ok, that looks not very self-explaining at the first sight. But no worries, this is normal as we have just dived into something completely unknown (if you do this exercise before the start of the main part of the problem set). We cover every detail together step by step in the main part of the problem set.

Now let's examine the first column. We see that it is named `shortnam`, whereas all observations in the column contain 3 letters - AFG, AGO, ARE and so on.


Quiz: What do 3 letters in the column `shortnam` mean?

[1]: Individual random code of every data point
[2]: First letters of researchers' names that gathered the respective data points
[3]: International country code
[4]: Chemical element symbol

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Makes sense")
```

Great, now we are familiar with at least one column in our data. Other columns are not that obvious, but we will not guess their meaning like we did for the first column. Let’s better load another piece of our data, which is named `Maketable2.dta`. Store it under the name `table_2` and show the "head" once again. 

```{r "14_3"}
# Assign the data maketable2.dta to the variable table_2 and show the first rows. Enter your code here:

```

Now compare the tables.


Quiz: Are there any similarities between the tables?

[1]: Same dimensions of the datasets
[2]: Same order of first columns
[3]: Some columns are similar
[4]: No

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Blind comparison")
```

Great, now we see that the data tables have something in common. One would agree that it could be difficult to get a complete picture of the data by looking at multiple tables at the same time, especially if there are many of them. 

Let's then load all necessary tables first.
You can just run the next chunk or press *check*:
```{r "14_4"}
table_4 = read.dta("maketable4.dta")
table_5 = read.dta("maketable5.dta")
table_6 = read.dta("maketable6.dta")
table_7 = read.dta("maketable7.dta")
```
No worries, we intentionally missed table_3 now, we do not need it so far. 

We have not shown all of them here to save some time, but you may always check them in the *data* tab. All loaded tables have many common columns, but also some unique ones. We can merge all these tables into one to eventually get an initial overview of the complete dataset. In order to do this, we will use the functions `Reduce()` and `merge`. `Reduce()` successively applies the specified function (first argument) to the vector or list of objects in the second argument. `merge()` identifies columns or rows that are common between the two different data frames and combines tables based on this intersection of data.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Reduce()")
```

```{r "14_5"}
# Replace the ___ and do not forget to delete the comment sign in the next row afterwards. Then show the first rows of the data with head().
data_0 = ___(___, list(table_1, table_2, table_4, table_5, table_7))
head(data_0)
```


Now we have quite a lot of variables, 31 to be precise, and seems like things only went from bad to worse, as variables’ meanings are still unclear. But no worries, we will be there in a second.

Let’s quickly rename the variables to make them more self-explaining. I will take advantage of the fact that I have already read the paper and give the variables names that make a little bit more sense. Just run the next chunk.

```{r "14_6"}
#We use function rename() to give new names to specified columns:
data_1 = rename(data_0,
    country = shortnam,
    exprop = avexpr,
    log_gdp_95 = logpgp95,
    base = baseco,
    log_mort = logem4,
    latit = lat_abst,
    log_work_output = loghjypl,
    eur_1900 = euro1900,
    cons_1st = cons1,
    cons_1900 = cons00a,
    cons_1990 = cons90,
    democ_1900 = democ00a,
    mort = extmort4,
    neo_europe = rich4,
    other_relig = no_cpm80,
    ex_colony = excolony,
    brit_col = f_brit,
    french_col = f_french,
    orig_french = sjlofr,
    mal_94 = malfal94,
    yell_fev = yellow,
    life_exp_95 = leb95,
    inf_mort_95 = imr95,
    coast_area = lt100km)
#Let's also use 0 instead of NAs for the variable "base" 
data_1$base[is.na(data_1$base)] = 0
#Show data
head(data_1)
```

Now we can hopefully see some more sense in these variables. 

Now it is time to make some intermediary adjustments to the data:
 - We have mentioned that there are dummy variables for continents, but it is clearly not the most visual way to present data. For descriptive analysis we would like to have one column with names of continents for each country, and for this we will use external data with more details. Nevertheless in a regression analysis we will get back to original dummies. 
 - Finally, it could be often quite convenient to reorder the columns so that the overall structure would be more logical. 

Let's make such adjustments, just run the chunk below.

```{r "14_7"}
#Load 2 files with coninent names for each country and PPP GDP in 2019
continents_data_raw = read.csv("continents.csv")
gdp2019_data_raw = read.csv("gdp2019.csv")
exprop21_data_raw = read.csv("Expropriation risk 2021.csv")


#Select only 2 necessary columns and rename them
continents_data = select(continents_data_raw, Three_Letter_Country_Code, Continent_Name) %>% rename(country = Three_Letter_Country_Code, continent = Continent_Name)
gdp2019_data = select(gdp2019_data_raw, Country.Code, X2019) %>% rename(country = Country.Code, gdp_2019 = X2019)
exprop21_data = select(exprop21_data_raw, Country, Expropriation.risk) %>% rename(country = Country, exprop_21 = Expropriation.risk)


#Merge original data with continents data and delete duplicating rows. We have to do it manually since some countries are assigned 2 continents (e.g. Armenia is assigned Europe and Asia at the same time).
data_merged_raw = merge(x = data_1, y = continents_data, by = "country", all.x = TRUE)
data_raw = data_merged_raw[-c(5,9,56,82,130,153),]

#Manually assign correct values to missing ones
data_raw[data_raw$country=="ZAR", "continent"] = "Africa"
data_raw[data_raw$country=="YUG", "continent"] = "Europe"
data_raw[data_raw$country=="ROM", "continent"] = "Europe"

#Add column with 2019 GDP data
data_merged_wGDP = merge(x = data_raw, y = gdp2019_data, by = "country", all.x = TRUE)

#Add logarithms of 2019 GDP for each country
data_merged_wGDP$log_gdp_19 = log(data_merged_wGDP$gdp_2019)


#Add column with 2021 expropriation risk data
data_merged_final = merge(x = data_merged_wGDP, y = exprop21_data, by = "country", all.x = TRUE)


#Subset necessary columns in a more convenient order
data = data_merged_final[c("country", "base", "ex_colony", "log_gdp_95", "log_gdp_19", "exprop", "exprop_21", "mort", "log_mort", "latit","continent", "africa", "asia", "neo_europe", "other", "eur_1900", "cons_1st", "cons_1900", "cons_1990", "democ_1900", "mal_94", "yell_fev", "life_exp_95", "inf_mort_95", "meantemp")]

#Now we can save this dataset in the .RDS format
saveRDS(data, file = "data.RDS")

```

Now we have a polished version of the dataset that we can analyze, show descriptive statistics and summarize. At this point, we have started (or will only start, if you did this exercise beforehand) the first exercise.

## Exercise 8: References

### Bibliography

- Acemoglu, D., (1995), Reward structures and the allocation of talent, European Economic Review, 39, issue 1, p. 17-33

- Acemoglu, D., Verdier, T., (1998). Property Rights, Corruption and the Allocation of Talent: A General Equilibrium Approach. Economic Journal, 108, issue 450, p. 1381-1403

- Acemoglu, D., Johnson, S., & Robinson, J. A. (2001). The Colonial Origins of Comparative Development: An Empirical Investigation. The American Economic Review, 91(5), 1369–1401

- Acemoglu, D., Johnson, S., & Robinson, J. A. (2005). A response to Albouy’s A Reexamination Based on Improved Settler Mortality Data, MIT Department of Economics Working Paper.

- Acemoglu, D., Johnson, S., & Robinson, J. A. (2006). Reply to the Revised (May 2006) Version of David Albouy’s The Colonial Origins of Comparative Development: An Investigation of the Settler Mortality Data. Unpublished, September, MIT and Harvard.

- Acemoglu, D., Johnson, S., & Robinson, J. A. (2012). Hither Thou Shalt Come, But No Further: Reply to “The Colonial Origins of Comparative Development: An Empirical Comment”. April, MIT and Harvard Working Paper 16966.

- Albouy, D. (2004). A Reexamination Based on Improved Settler Mortality Data, University of California — Berkeley, December

- Albouy, D. (2012). The Colonial Origins of Comparative Development: An Empirical Investigation: Comment, American Economic Review, 102(6), 3059-76

- Besley, T. (1995). Property Rights and Investment Incentives: Theory and Evidence from Ghana. Journal of Political Economy, October 1995, 103(5), pp. 903-37.

- Curtin, P. D. (1964). The image of Africa. Madison, WI: University of Wisconsin Press

- Curtin, P. D. (1968). Epidemiology and the Slave Trade. Political Science Quarterly, 83(2), 190–216

- Curtin, P. D. (1989). Death by migration: Europe's encounter with the tropical world in the 19th Century. New York: Cambridge University Press

- Curtin, P. D. (1989). Disease and empire: The health of European troops in the conquest of Africa. New York: Cambridge University Press

- Denoon, D. (1983). Settler capitalism: The dynamics of dependent development in the Southern Hemisphere. Oxford [Oxfordshire: Clarendon Press.

- Diamond, J. M. (1997). Guns, germs and steel: The fate of human societies. New York: W.W. Norton & Co.

- Gallup, J. L., Sachs, J. D., & Mellinger, A. D. (1998). Geography and Economic Development. International Regional Science Review, 22(2), 179–232.

- Glaeser, E. L., La Porta, R., Lopez-de-Silanes, F., & Shleifer, A. (2004). Do institutions cause growth?. Journal of economic Growth, 9(3), 271-303.

- Greene W. H. (2020). Econometric Analysis: Global Edition, 8th Edition. Stern School of Business, New York University

- Hall, R., Jones, C., (1999). Why Do Some Countries Produce So Much More Output per Worker than Others?", Quarterly Journal of Economics, February 1999, 114(1), pp. 83-116.

- Johnson, S., McMillan, J., Woodruff, C. (1999). Property Rights and Finance. Unpublished working paper, Massachusetts Institute of Technology and University of California, San Diego

- Jones, E. L. (1981). The European miracle: Environments, economies and geopolitics in the history of Europe and Asia. Cambridge: Cambridge University Press

- Knack, S., Keefer, P. (1995). Institutions and Economic Performance: Cross-Country Tests Using Alternative Measures. Economics and Politics, 7(3), pp. 207-27

- La Porta, R., Lopez-de-Silanes, F., Shleifer, A. and Vishny, R. W. (1999). The Quality of Government. Journal of Law, Economics, and Organization, 15(1), pp. 222-79

- Mauro, P. (1995). Corruption and Growth. Quarterly Journal of Economics, 110 (3), pp. 681-712

- Mazingo, C. (1999). Effects of Property Rights on Economic Activity: Lessons from the Stolypin Land Reform. Unpublished manuscript, Massachusetts Institute of Technology

- McArthur, J. W., Sachs, J. D. (2001). Institutions and Geography: Comment on Acemoglu, Johnson and Robinson (2000). National Bureau of Economic Research (Cambridge, MA) Working Paper No. 8114

- Montesquieu C. S. (1989). The spirit of the laws. New York: Cambridge University Press [1748]

- North, D. C. (1981). Structure and change in economic history. New York: W.W. Norton & Co.

- Oldstone, Michael B. A. (1998). Viruses, plagues, and history. New York: Oxford University Press

- Rodrik, D. (1999). Where Did All the Growth Go?", Journal of Economic Growth, 4(4), pp. 385-412

- Stock J, Yogo M. (2005). Testing for Weak Instruments in Linear IV Regression. In: Andrews DWK Identification and Inference for Econometric Models, New York: Cambridge University Press, pp. 80-108

- Wooldridge, J.M. (2018) Introductory Econometrics: A Modern Approach. 7th Edition. Cengage

### R packages

- Arel-Bundock, V., Gassen, J. et al. (2022): modelsummary. Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready. R package version 0.9.5. URL: https://cran.r-project.org/web/packages/modelsummary/modelsummary.pdf 

- Auguie, B., Antonob, A. (2017): gridextra. Miscellaneous Functions for ``Grid'' Graphics. R package version 2.3. URL: https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf

- Barrett, M. (2021): regtools. Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.4. URL: https://cran.r-project.org/web/packages/ggdag/ggdag.pdf

- Fox, J., Kleiber, C., Zeileis, A., Kuschnig, N. (2021): ivreg. Instrumental-Variables Regression by '2SLS', '2SM', or '2SMM', with Diagnostics. R package version 0.6-1. URL: https://cran.r-project.org/web/packages/ivreg/ivreg.pdf

- Hlavac, M. (2018): stargazer. Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. URL: https://cran.r-project.org/web/packages/stargazer/stargazer.pdf

- Kranz, S. (2020). RTutor: Interactive R problem sets with automatic testing of solutions and automatic hints. R package version 2020.11.25. URL: https://github.com/skranz/RTutor

- Matloff, N. (2019): regtools. Regression and Classification Tools. R package version 1.1.0. URL: https://cran.r-project.org/web/packages/regtools/regtools.pdf

- Phillips, N. (2017): yarrr. A Companion to the e-Book ``YaRrr!: The Pirate's Guide to R''. R package version 0.1.5. URL: https://cran.r-project.org/web/packages/yarrr/yarrr.pdf

- Sievert, C., Parmer, C., Hocking, T. et al. (2021): plotly. Create Interactive Web Graphics via 'plotly.js'. R package version 4.10.0. URL: https://cran.r-project.org/web/packages/yarrr/yarrr.pdf

- R Core Team (2022): foreign. Read Data Stored by Minitab, S, SAS, SPSS, Stata, Systat, Weka, dBase,…. R package version 0.8-82. URL: https://cran.r-project.org/web/packages/foreign/foreign.pdf

- Waring, E. et al. (2021): skimr. Compact and Flexible Summaries of Data. R package version 2.1.3. URL: https://cran.r-project.org/web/packages/skimr/skimr.pdf

- Wickham, H., Chang, W. (2021): ggplot2. Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 3.3.5. URL: https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf.

- Wickham, H., Francois, R., Henry, L. and Müller, K. (2021): dplyr. A Grammar of Data Manipulation. R package version 1.0.7. URL: https://cran.r-project.org/web/packages/dplyr/dplyr.pdf.

- Wickham, H., RStudio (2021): tidyverse. Easily Install and Load the 'Tidyverse'. R package version 1.3.1. URL: https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf


### Data

- Original data from the D. Acemoglu et al. (2001) paper: https://economics.mit.edu/faculty/acemoglu/data/ajr2001

- Modern data for expropriation risk: https://credendo.com/en/country-risk

- Modern PPP GDP per capita data: https://data.worldbank.org/indicator/NY.GDP.MKTP.PP.CD
